#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 EDF SA
# Contact:
#       CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#       1, Avenue du General de Gaulle
#       92140 Clamart
#
# Authors: CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#
# This file is part of puppet-hpc.
#
# HPCStats is free software: you can redistribute in and/or
# modify it under the terms of the GNU General Public License,
# version 2, as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public
# License along with HPCStats. If not, see
# <http://www.gnu.org/licenses/>.

import os
import argparse
import configparser
from io import StringIO
import tarfile
import sys
import logging
logger = logging.getLogger(__name__)
import tempfile
import shutil
import boto
import boto.s3
import boto.s3.connection

class AppConf():
    """Runtime configuration class."""

    def __init__(self):

        self.debug = False
        self.conf_file = None
        self.cluster = None
        self.environment = None
        self.version = None

        self.mode = None

        ## Common parameters
        self.destination_root = None

        ## S3 parameters
        self.s3_access_key = None
        self.s3_secret_key = None
        self.s3_bucket_name = None
        self.s3_host = None
        self.s3_port = None

        # action

        self.full_tmp_cleanup = False

        # paths

        self.conf_puppet = None
        self.conf_hiera = None
        # This one is hard-coded, there is no configuration parameter to
        # change it since it would be irrelevant to change it.
        self.conf_environment = 'environment.conf'
        self.facts_private = None
        self.dir_modules_generic = None
        self.dir_modules_private = None
        self.dir_manifests_generic = None
        self.dir_manifests_private = None
        self.dir_hieradata_generic = None
        self.dir_hieradata_private = None
        self.dir_files_private = None


        self.dir_tmp = None
        self.dir_tmp_gen = None

    def dump(self):
        logger.debug("runtime configuration dump:")
        logger.debug("- debug: %s", str(self.debug))
        logger.debug("- conf_file: %s", str(self.conf_file))
        logger.debug("- cluster: %s", str(self.cluster))
        logger.debug("- environment: %s", str(self.environment))
        logger.debug("- version: %s", str(self.version))
        logger.debug("- mode: %s", str(self.mode))
        logger.debug("- destination_root: %s", str(self.destination_root))
        logger.debug("- destination: %s", str(self.destination))
        logger.debug("- dir_tmp: %s", str(self.dir_tmp))
        logger.debug("- conf_puppet: %s", str(self.conf_puppet))
        logger.debug("- conf_hiera: %s", str(self.conf_hiera))
        logger.debug("- facts_private: %s", str(self.facts_private))
        logger.debug("- dir_modules_generic: %s", str(self.dir_modules_generic))
        logger.debug("- dir_modules_private: %s", str(self.dir_modules_private))
        logger.debug("- dir_manifests_generic: %s", str(self.dir_manifests_generic))
        logger.debug("- dir_manifests_private: %s", str(self.dir_manifests_private))
        logger.debug("- dir_hieradata_generic: %s", str(self.dir_hieradata_generic))
        logger.debug("- dir_hieradata_private: %s", str(self.dir_hieradata_private))
        logger.debug("- dir_files_private: %s", str(self.dir_files_private))
        logger.debug("- s3_access_key: %s", str(self.s3_access_key))
        logger.debug("- s3_secret_key: %s", str(self.s3_secret_key))
        logger.debug("- s3_bucket_name: %s", str(self.s3_bucket_name))
        logger.debug("- s3_port: %s", str(self.s3_port))
        logger.debug("- s3_host: %s", str(self.s3_host))

    @property
    def archive(self):
        return os.path.join(self.dir_tmp_gen, 'puppet-config-environment.tar.xz')

    @property
    def conf_environment_gen(self):
        """Path where environment.conf is generated."""
        return os.path.join(self.dir_tmp_gen, self.conf_environment)

    @property
    def destination(self):
        return os.path.join(self.destination_root, self.environment, self.version)

conf = AppConf()  # global runtime configuration object

def parse_conf():
     """Parse configuration file and set runtime configuration accordingly.
        Here are defined default configuration file parameters."""
     defaults = StringIO(
       "[global]\n"
       "cluster = unknown\n"
       "environment = production\n"
       "version = latest\n"
       "mode = posix\n"
       "destination = /var/www/html/hpc-config\n"
       "[s3]\n"
       "access_key = XXXXXXXX\n"
       "secret_key = YYYYYYYYYYYYYYYY\n"
       "bucket_name = system\n"
       "host = rgw.service.virtual\n"
       "port = 7480\n"
       "[paths]\n"
       "tmp = /tmp/puppet-config-push\n"
       "puppethpc = puppet-hpc\n"
       "privatedata = hpc-privatedata\n"
       "puppet_conf = ${privatedata}/puppet-config/${global:cluster}/puppet.conf\n"
       "hiera_conf = ${privatedata}/puppet-config/${global:cluster}/hiera.yaml\n"
       "facts_private = ${privatedata}/puppet-config/${global:cluster}/hpc-config-facts.yaml\n"
       "modules_generic = ${puppethpc}/puppet-config/cluster,${puppethpc}/puppet-config/modules,/usr/share/puppet/modules\n"
       "modules_private = ${privatedata}/puppet-config/${global:cluster}/modules\n"
       "manifests_generic = ${puppethpc}/puppet-config/manifests\n"
       "manifests_private = ${privatedata}/puppet-config/${global:cluster}/manifests\n"
       "hieradata_generic = ${puppethpc}/hieradata\n"
       "hieradata_private = ${privatedata}/hieradata\n"
       "files_private = ${privatedata}/files/${global:cluster}\n")
     parser = configparser.ConfigParser()
     parser._interpolation = configparser.ExtendedInterpolation()
     parser.readfp(defaults)
     parser.read(conf.conf_file)
     conf.cluster = parser.get('global', 'cluster')
     conf.environment = parser.get('global', 'environment')
     conf.version = parser.get('global', 'version')
     conf.mode = parser.get('global', 'mode')
     conf.destination_root = parser.get('global', 'destination')
     conf.dir_tmp = parser.get('paths', 'tmp')
     conf.conf_puppet = parser.get('paths', 'puppet_conf')
     conf.conf_hiera = parser.get('paths', 'hiera_conf')
     conf.facts_private = parser.get('paths', 'facts_private')
     conf.dir_modules_generic = parser.get('paths', 'modules_generic').split(',')
     conf.dir_modules_private = parser.get('paths', 'modules_private')
     conf.dir_manifests_generic = parser.get('paths', 'manifests_generic')
     conf.dir_manifests_private = parser.get('paths', 'manifests_private')
     conf.dir_hieradata_generic = parser.get('paths', 'hieradata_generic')
     conf.dir_hieradata_private = parser.get('paths', 'hieradata_private')
     conf.dir_files_private = parser.get('paths', 'files_private')
     conf.s3_access_key = parser.get('s3', 'access_key')
     conf.s3_secret_key = parser.get('s3', 'secret_key')
     conf.s3_bucket_name = parser.get('s3', 'bucket_name')
     conf.s3_host = parser.get('s3', 'host')
     conf.s3_port = int(parser.get('s3', 'port'))

def parse_args():
    """Parses CLI args, then set debug flag and configuration file path in
       runtime configuration accordingly, and returns the args."""
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--debug',
                        help='Enable debug mode',
                        action='store_true')
    parser.add_argument('-c', '--conf',
                        help='Path to the configuration file',
                        nargs='?',
                        default='/etc/hpc-config/push.conf')
    parser.add_argument('-e', '--environment',
                        help='Name of the pushed environment',
                        nargs='?')
    parser.add_argument('-V', '--version',
                        help='Version of the pushed config',
                        nargs='?')
    parser.add_argument('--full-tmp-cleanup',
                        help='Full tmp dir cleanup.',
                        action='store_true')
    args = parser.parse_args()

    if args.debug:
        conf.debug = True
    if args.conf:
        conf.conf_file = args.conf

    return args

def setup_logger():

    if conf.debug is True:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def override_conf(args):
    """Override configuration files parameters with args values."""
    if args.environment:
        conf.environment = args.environment
    if args.version:
        conf.version = args.version
    if args.full_tmp_cleanup:
        conf.full_tmp_cleanup = True

def mktmpd():
    """Make tmp generate dir and its parents."""
    if not os.path.isdir(conf.dir_tmp):
        os.makedirs(conf.dir_tmp)
    conf.dir_tmp_gen = tempfile.mkdtemp(dir=conf.dir_tmp)

def build_tarball():

    logger.info("creating archive %s", conf.archive)
    tar = tarfile.open(name=conf.archive, mode='w:xz', dereference=True)

    # generic modules
    seen_modules = []
    for modulesdir in conf.dir_modules_generic:

        if os.path.exists(modulesdir) and \
           os.path.isdir(modulesdir):

            # detect and raise error in case of module conflict
            new_modules = os.listdir(modulesdir)
            intersect = list(set(seen_modules) & set(new_modules))
            if len(intersect):
                logger.error("modules conflict in %s: %s", modulesdir, str(intersect))
                sys.exit(1)
            seen_modules += new_modules

            logger.debug("adding generic modules dir %s: %s", modulesdir, str(new_modules))
            tar.add(modulesdir, arcname=os.path.join(conf.environment, 'modules_generic'))

    # private modules
    if os.path.exists(conf.dir_modules_private) and \
       os.path.isdir(conf.dir_modules_private):
        logger.debug("adding private modules dir %s", conf.dir_modules_private)
        tar.add(conf.dir_modules_private, arcname=os.path.join(conf.environment, 'modules_private'))
    # generic manifests
    if os.path.exists(conf.dir_manifests_generic) and \
       os.path.isdir(conf.dir_manifests_generic):
        logger.debug("adding generic manifests dir %s", conf.dir_manifests_generic)
        tar.add(conf.dir_manifests_generic, arcname=os.path.join(conf.environment, 'manifests'))
    # private manifests
    if os.path.exists(conf.dir_manifests_private) and \
       os.path.isdir(conf.dir_manifests_private):
        logger.debug("adding private manifests dir %s", conf.dir_manifests_private)
        tar.add(conf.dir_manifests_private, arcname=os.path.join(conf.environment, 'manifests'))
    # generic hieradata
    if os.path.exists(conf.dir_hieradata_generic) and \
       os.path.isdir(conf.dir_hieradata_generic):
        logger.debug("adding generic hieradata dir %s", conf.dir_hieradata_generic)
        tar.add(conf.dir_hieradata_generic, arcname=os.path.join(conf.environment, 'hieradata', 'generic'))
    # private hieradata
    if os.path.exists(conf.dir_hieradata_private) and \
       os.path.isdir(conf.dir_hieradata_private):
        logger.debug("adding private hieradata dir %s", conf.dir_hieradata_private)
        tar.add(conf.dir_hieradata_private, arcname=os.path.join(conf.environment, 'hieradata', 'private'))

    logger.debug("adding environment conf")
    tar.add(conf.conf_environment_gen, arcname=os.path.join(conf.environment, conf.conf_environment))

    tar.close()

def gen_env_conf():

    with open(conf.conf_environment_gen, 'w+') as env_f:
        env_f.write("modulepath=modules_private:modules_generic\n")
        env_f.write("manifest=manifests/cluster.pp\n")

def _push_posix():

    logger.info("posix push: pushing data in %s", conf.destination)

    if not os.path.isdir(conf.destination):
        logger.debug("posix push: create destination dir %s", conf.destination)
        os.makedirs(conf.destination, exist_ok=True)

    logger.debug("posix push: copying tarball")
    shutil.copy(conf.archive, conf.destination)

    dir_files = os.path.join(conf.destination, 'files')
    if os.path.isdir(dir_files):
        logger.debug("posix push: removing push private files dir %s", dir_files)
        shutil.rmtree(dir_files)

    logger.debug("posix push: copying private files")
    shutil.copytree(conf.dir_files_private, dir_files)
    logger.debug("posix push: copying puppet conf")
    shutil.copy(conf.conf_puppet, conf.destination)
    logger.debug("posix push: copying hiera conf")
    shutil.copy(conf.conf_hiera, conf.destination)
    logger.debug("posix push: copying private facts")
    shutil.copy(conf.facts_private, conf.destination)


def _s3_upload(source_path, bucket, destination_path, clean=True):
    logger.debug('Upload path: %s', source_path)
    #max size in bytes before uploading in parts. between 1 and 5 GB recommended
    max_size = 20 * 1000 * 1000
    #size of parts when uploading in parts
    part_size = 6 * 1000 * 1000

    # List files to upload
    upload_file_paths = []
    if os.path.isfile(source_path):
        relative_path = os.path.basename(source_path)
        upload_file_paths.append(relative_path)
    for (current_dir, subdirs, filenames) in os.walk(source_path):
        for filename in filenames:
            absolute_path = os.path.join(current_dir, filename)
            # remove the source path and first /
            relative_path = absolute_path[(len(source_path)+1):]
            upload_file_paths.append(relative_path)

    # Upload the files
    for file_path in upload_file_paths:
        # Determine file paths
        if os.path.isfile(source_path):
            source_file_path = source_path
        else:
            source_file_path = os.path.join(source_path, file_path)
            logger.debug("Source file path is: %s (%s, %s)", source_file_path, source_path, file_path)
        dest_file_path = os.path.join(destination_path, file_path)
        logger.debug("Dest file path is: %s (%s, %s)", dest_file_path, destination_path, file_path)

        # Create remote directory if necessary
        dest_dir_name = os.path.dirname(dest_file_path) + "/"
        dest_dir = bucket.get_key(dest_dir_name)
        if dest_dir is None:
            logger.debug("S3 upload: Creating directory %s", dest_dir_name)
            dest_dir = bucket.new_key(dest_dir_name)
            dest_dir.set_contents_from_string('', policy='public-read')
  
        # Determine upload method
        filesize = os.path.getsize(source_file_path)
        if filesize > max_size:
            logger.debug("S3 upload: multipart upload for %s", file_path)
            mp = bucket.initiate_multipart_upload(dest_file_path, policy='public-read')
            fp = open(source_file_path, 'rb')
            fp_num = 0
            while fp.tell() < filesize:
                fp_num += 1
                logger.debug("S3 upload: uploading part %i", fp_num)
                mp.upload_part_from_file(fp, fp_num, size=part_size)

            mp.complete_upload()
        else:
            logger.debug("S3 upload: singlepart upload for %s", file_path)
            k = boto.s3.key.Key(bucket)
            k.key = dest_file_path
            bytes_written = k.set_contents_from_filename(source_file_path, policy='public-read')
            logger.debug("S3 upload: %d/%d bytes written for %s", 
                bytes_written, filesize, file_path)


def _push_s3():
    logger.info("S3 push: pushing data in bucket %s", conf.s3_bucket_name)

    conn = boto.connect_s3(
        aws_access_key_id=conf.s3_access_key,
        aws_secret_access_key=conf.s3_secret_key,
        host=conf.s3_host,
        port=conf.s3_port,
        is_secure=False,
        calling_format=boto.s3.connection.OrdinaryCallingFormat(),
    )
    bucket = conn.get_bucket(conf.s3_bucket_name)
    bucket.set_acl('public-read')

    logger.debug("S3 push: Cleaning destination %s", conf.destination)
    keys = bucket.list(prefix=conf.destination)
    counter = 0
    for key in keys:
        key.delete()
        counter += 1
    logger.debug("S3 push: Deleted %s keys", counter)

    logger.debug("S3 push: copying tarball")
    _s3_upload(conf.archive, bucket, conf.destination)

    logger.debug("S3 push: copying private files")
    dir_files = os.path.join(conf.destination, 'files')
    _s3_upload(conf.dir_files_private, bucket, dir_files)
    logger.debug("S3 push: copying puppet conf")
    _s3_upload(conf.conf_puppet, bucket, conf.destination)
    logger.debug("S3 push: copying hiera conf")
    _s3_upload(conf.conf_hiera, bucket, conf.destination)
    logger.debug("S3 push: copying private facts")
    _s3_upload(conf.facts_private, bucket, conf.destination)


def push():

    if conf.mode == 'posix':
        _push_posix()
    elif conf.mode == 's3':
        _push_s3()
    else:
        logger.error("unsupported push mode %s", conf.mode)
        sys.exit(1)

def cleanup_run():
    """Remove the run tmp dir."""

    logger.debug("removing run tmp dir %s", conf.dir_tmp_gen)
    shutil.rmtree(conf.dir_tmp_gen)

def cleanup_full():
    """Remove the full app tmp dir."""
    if not os.path.isdir(conf.dir_tmp):
        logger.info("app tmp dir %s does not exists, nothing to remove.", conf.dir_tmp)
    else:
        logger.info("removing app tmp dir %s", conf.dir_tmp)
        shutil.rmtree(conf.dir_tmp)

def main():

    #
    # init
    #
    args = parse_args()
    setup_logger()
    parse_conf()
    override_conf(args)
    conf.dump()

    #
    # run
    #
    if conf.full_tmp_cleanup:
        cleanup_full()
    else:
        mktmpd()
        gen_env_conf()
        build_tarball()
        push()
        cleanup_run()
 
if __name__ == '__main__':
    main()
