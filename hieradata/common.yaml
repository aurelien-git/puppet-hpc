###### Global Organization settings ######
# These  values should be redefined for your site
cluster_name:             'mycluster'
cluster_prefix:           'my'
domain:                   'hpc.example.com'
cluster_repo_originator:  'TeamHPC'
cluster_repo_gpg_key:     '01234567'
cluster_decrypt_password: 'password'
# These values might be redefined for your site
admin_dir:      '/var/admin'
website_dir:    "%{hiera('admin_dir')}/public/http"
preseeddir:     '/var/cache/debconf'
libcalibre:     '/var/lib/calibre'
home_dir:       '/home'

###### NTP
ntp::iburst_enable: false
ntp::disable_auth: true
ntp::disable_monitor: false
ntp::driftfile: '/var/lib/ntp/ntp.drift'
# IPv6 is disabled in /etc/default, so we don't include
# localhost IPv6 addresses in restrict
ntp::restrict: 
  - 'default kod notrap nomodify nopeer'
  - "%{hiera('net::allloc::ipnetwork')} mask %{hiera('net::allloc::netmask')} notrap nomodify"
  - '127.0.0.1'
profiles::ntp::srv_opts:
  NTPD_OPTS: "'-4 -g'"
profiles::ntp::client::site_preferred_servers:
  - "%{hiera('cluster_prefix')}%{::my_ntp_server}1"
profiles::ntp::client::site_servers:
  - "%{hiera('cluster_prefix')}%{::my_ntp_server}2"

##### SSMTP
profiles::ssmtp::config_options:
  mailhub:       "%{hiera('cluster_prefix')}%{::my_postfix_relay}1"
  rewritedomain: "%{hiera('cluster_name')}.%{hiera('domain')}"

##### POSTFIX
profiles::postfix::relay::config_options:
  myhostname:          "%{hostname}"
  mydomain:            "%{hiera('cluster_name')}.%{hiera('domain')}"

##### TFTP
profiles::tftp::server::tftp_opts:
  TFTP_DIRECTORY: "\"%{hiera('admin_dir')}/public/tftp\""

##### SSH
profiles::openssh::server::config_augeas:
  - 'set MaxStartups 8192'
  - 'set PermitRootLogin yes'
  - 'set X11UseLocalhost no'
openssh::server::hostkeys_source_dir: "%{hiera('private_files_dir')}/hostkeys"
openssh::server::decrypt_passwd:      "%{hiera('cluster_decrypt_password')}"
openssh::client::root_key_enc:        "%{hiera('private_files_dir')}/rootkeys/id_rsa_root.enc"
openssh::client::decrypt_passwd:      "%{hiera('cluster_decrypt_password')}"


##### Environment
profiles::environment::authorized_users_group: "%{hiera('user_group')}"

##### Codes packages
profiles::environment::codes::packages:
  - "%{hiera('cluster_name')}-hpc-%{puppet_role}"
## Values below should be overriden by roles
profiles::access::production_options: []
profiles::access::maintenance_options: []

##### Access Rules #####
profiles::access::maintenance_mode: false

##### SSSD
profiles::auth::client::enable_kerberos: false
profiles::auth::client::sssd_options_general:
  config_file_version:  '2'
  reconnection_retries: '3'
  sbus_timeout:         '30'
  services:             'nss,pam'
  domains:              "%{hiera('cluster_name')}"
profiles::auth::client::sssd_options_nss:
  filter_groups:        'root'
  filter_users:         'root'
  reconnection_retries: '3'
profiles::auth::client::sssd_options_pam:
  reconnection_retries:           '3'
  offline_credentials_expiration: '1'
profiles::auth::client::krb5_server:                    "%{hiera('kdc_primary')}, %{hiera('kdc_secondary')}"
profiles::auth::client::krb5_realm:                     "%{hiera('krb5_realm')}"
profiles::auth::client::krb5_store_password_if_offline: 'true'
kerberos::keytab_source_dir:                            "%{hiera('private_files_dir')}/auth/keytabs"
kerberos::decrypt_passwd:                               "%{hiera('cluster_decrypt_password')}"

###### Openldap
profiles::auth::replica::ldif_directory:   "%{hiera('libcalibre')}/slapdcfg"
profiles::auth::replica::ldif_file:        'config_replica.ldif'
profiles::auth::replica::directory_source: "%{hiera('private_files_dir')}/auth"
certificates::directory_source:            "%{hiera('private_files_dir')}/auth"
certificates::certificates_directory:      '/etc/ldap/certificates'
certificates::certificate_file:            "%{hiera('cluster_name')}_ldap.crt"
certificates::key_file:                    "%{hiera('cluster_name')}_ldap.key"
certificates::decrypt_passwd:              "%{hiera('cluster_decrypt_password')}"
openldap::replica::decrypt_passwd:         "%{hiera('cluster_decrypt_password')}"


##### HTTP
apache::apache_version:          '2.4'
profiles::http::port:            80
profiles::http::error_log_file:  'error.log'
profiles::http::log_level:       'warn'
profiles::http::serveradmin:     'webmaster@localhost'
profiles::http::scriptalias:     "%{hiera('profiles::http::mirror::docroot')}/cgi-bin"
profiles::http::mirror::docroot: "%{hiera('website_dir')}/mirror"

profiles::http::secret::port:    '1216'
profiles::http::secret::docroot: "%{hiera('admin_dir')}/restricted/secrets"

profiles::http::diskless::port:    '3137'
profiles::http::diskless::docroot: "%{hiera('website_dir')}/diskless"

###### P2P
profiles::p2p::tracker_path: "%{hiera('profiles::http::diskless::docroot')}"

##### Munge
munge::auth_key_source: "%{hiera('private_files_dir')}/munge/munge.key.enc"
munge::decrypt_passwd:  "%{hiera('cluster_decrypt_password')}"

###### SLURM
profiles::jobsched::user:  'slurm'
slurm_primary_server:      "%{hiera('cluster_prefix')}%{::my_jobsched_server}1"
slurm_secondary_server:    "%{hiera('cluster_prefix')}%{::my_jobsched_server}2"
slurm_cluster_name:        "%{hiera('cluster_name')}"
slurmd_user:               'root'
slurm::exec::enable_cgroup: true
profiles::jobsched::slurm_config_options:
  ClusterName: 
    value:   "%{hiera('slurm_cluster_name')}"
    comment: "The name by which this SLURM managed cluster is known in the accounting database"
  ControlMachine:
    value:   "%{hiera('slurm_primary_server')}"
    comment: "Hostname of the machine where SLURM control functions are executed"
  BackupController:
    value:   "%{hiera('slurm_secondary_server')}"
    comment: "Machine where SLURM control functions are to be executed if ControlMachine fails"
  SlurmUser:
    value:   "%{hiera('profiles::jobsched::user')}"
    comment: "The name of the user that the slurmctld daemon executes as"
  SlurmdUser:
    value:   "%{hiera('slurmd_user')}"
    comment: "The name of the user that the slurmd daemon executes as"
  DisableRootJobs:
    value:   "YES"
    comment: "If set to 'YES' then user root will be prevented from running any jobs"
  CacheGroups:
    value:   "0"
    comment: "If set to 1 the slurmd daemon will cache /etc/groups entries. This can improve performance for highly parallel jobs"
  GroupUpdateForce:
    value:   "1"
    comment: "If set to 1 then information about which users are members of groups allowed to use a partition will be updated periodically"
  GroupUpdateTime:
    value:   "600"
    comment: "Controls how frequently information about which users are members of groups allowed to use a partition will be updated"
  UsePAM:
    value:   "1"
    comment: "If set to 1 authentication by PAM will be enabled"
  SwitchType:
    value:   "switch/none"
    comment: "Identifies the type of switch or interconnect used for application communications. Acceptable values include 'switch/none' and 'switch/elan'"
  TreeWidth:
    value:   "600"
    comment: "Slurmd daemons use a virtual tree network for communications. IMPORTANT: configure TreeWidth to a number at least as large as the maximum node count!!"
  HealthCheckProgram:
    value:   "/usr/sbin/nhc"
    comment: "Script to execute as user root periodically on all compute nodes. This may be used to verify the node is fully operational and DRAIN the node if a problem is detected."
  HealthCheckInterval:
    value:   "300"
    comment: "The interval in seconds between executions of HealthCheckProgram. If value is zero execution will be disabled"
  InactiveLimit:
    value:   "0"
    comment: "IMPORTANT : Script /usr/local/sbin/slurm_srun_prolog.visu require set up this value to zero"
  MessageTimeout:
    value:   "30"
    comment: "Time permitted for a round-trip communication to complete in seconds"
  ResvOverRun:
    value:   "10"
    comment: "Describes how long (in minutes] a job already running in a reservation should be permitted to execute after the end time of the reservation has been reached"
  MaxJobCount:
    value:   "5000"
    comment: "The maximum number of jobs SLURM can have in its active database at one time"
  SlurmctldTimeout:
    value:   "60"
    comment: "The interval in seconds that the backup controller waits for the primary controller to respond before assuming control"
  SlurmdTimeout:
    value:   "120"
    comment: "The interval in seconds that the SLURM controller waits for slurmd to respond before configuring that node's state to DOWN"
  SchedulerType:
    value:   "sched/backfill"
    comment: "sched/wiki or sched/wiki2: for the Wiki interface to the Maui Schedule"
  SelectType:
    value:   "select/cons_res"
    comment: "select/cons_res: The resources within a node are individually allocated as consumable resources. Note that whole nodes can be allocated to jobs for selected partitions by using the Shared=Exclusive option. Defines specific subsystems which should provide more detailed event logging. Valid subsystems available today (with more to come) include:"
  SelectTypeParameters:
    value:   "CR_CPU_Memory"
    comment: ""
  PropagateResourceLimits:
    value:   "NONE"
    comment: "The following limit names are supported by SLURM: ALL NONE AS CORE CPU DATA FSIZE MEMLOCK NOFILE NPROC RSS STACK. A list of comma separated resource limit names to exclude"
  EnforcePartLimits:
    value:   "YES"
    comment: "If set to 'YES' then jobs which exceed a partition's size and/or time limits will be rejected at submission time"
  JobSubmitPlugins:
    value:   "lua"
    comment: "A comma delimited list of job submission plugins to be used. The specified plugins will be executed in the order listed."
  MaxStepCount:
    value:   "100000"
    comment: "The maximum number of steps that any job can initiate"
  MaxTasksPerNode:
    value:   "28"
    comment: "Maximum number of tasks SLURM will allow a job step to spawn on a single node"
  TaskPlugin:
    value:   "task/cgroup"
    comment: "'task/affinity' 'task/cgroup' or 'task/none'"
  ProctrackType:
    value:   "proctrack/cgroup"
    comment: "Identifies the plugin to be used for process tracking"
  PriorityType:
    value:   "priority/basic"
    comment: "This specifies the plugin to be used in establishing a jobs scheduling priority. Supported values are 'priority/basic' and 'priority/multifactor'"
  AccountingStorageType:
    value:   "accounting_storage/slurmdbd"
    comment: "'accounting_storage/filetxt' 'accounting_storage/mysql' 'accounting_storage/none' 'accounting_storage/pgsql' and 'accounting_storage/slurmdbd'"
  AccountingStorageHost:
    value:   "%{hiera('slurm_primary_server')}"
  AccountingStorageBackupHost:
    value:   "%{hiera('slurm_secondary_server')}"
  TrackWCKey:
    value:   "yes"
    comment: "If you want to track wckeys (Workload Characterization Key) of users."
  JobAcctGatherType:
    value:   "jobacct_gather/linux"
    comment: "The job accounting mechanism type. Acceptable values at present include 'jobacct_gather/aix' 'jobacct_gather/linux' and 'jobacct_gather/none'"
  JobAcctGatherFrequency:
    value:   "60"
    comment: "A value of zero disables real the periodic job sampling and provides accounting information only on job termination. "
  SlurmctldDebug:
    value:   "3"
    comment: "The level of detail to provide slurmctld daemon's logs. Values from 0 to 9 are legal"
  SlurmdDebug:
    value:   "3"
    comment: "The level of detail to provide slurmd daemon's logs. Values from 0 to 9 are legal"
  ReturnToService:
    value:   "2"
    comment: "Controls when a DOWN node will be returned to service. 1 = A DOWN node will become available for use upon registration with a valid configuration only if it was set DOWN due to being non-responsive"

##### Slurm DBD database
galera_base_name:             "%{hiera('cluster_prefix')}%{::my_db_server}"
mariadb::galera_conf_options:
  mysqld:
    binlog_format:            'ROW'
    default-storage-engine:   'innodb'
    innodb_autoinc_lock_mode: '2'
    query_cache_size:         '0'
    query_cache_type:         '0'
    bind-address:             '0.0.0.0'
    wsrep_provider:           '/usr/lib/galera/libgalera_smm.so'
    wsrep_cluster_name:       '"galera_cluster"'
    wsrep_cluster_address:    "\"gcomm://%{hiera('galera_base_name')}1,%{hiera('galera_base_name')}2\""
    wsrep_sst_method:         'rsync'
slurm::dbd::config_options:
  DbdHost:           "%{hiera('slurm_primary_server')}"
  DbdBackupHost:     "%{hiera('slurm_secondary_server')}"
  SlurmUser:         "%{hiera('profiles::jobsched::user')}"
  StorageHost:       'localhost'
  StorageUser:       'slurm'
  StoragePass:       "%{hiera('slurmdbd_slurm_db_password')}"
slurm::dbd::db_options:
  db:
    hosts:       'localhost'
    user:        'debian-sys-maint'
    password:    "%{hiera('mariadb::mysql_root_pwd')}"
  passwords:
    slurm:       "%{hiera('slurmdbd_slurm_db_password')}"
    slurmro:     "%{hiera('slurmdbd_slurmro_db_password')}"
  hosts:
    controllers: "%{hiera('slurm_primary_server')},%{hiera('slurm_secondary_server')}"
    admins:      "%{hiera('cluster_prefix')}admin1"

##### NHC
profiles::warewulf_nhc::config_options:
  rm:
    comment: 'Explicitly instruct NHC to assume the Resource Manager'
    nodes:   '*'
    command: 'export NHC_RM=slurm'
  mark:
    comment: 'Do not mark nodes offline'
    nodes:   '*'
    command: 'export MARK_OFFLINE=1'
  debug:
    comment: 'Activate debugging mode'
    nodes:   '*'
    command: 'export DEBUG=0'
  watchdog:
    comment: 'Set watchdog timer to 15 seconds'
    nodes:   '*'
    command: 'export TIMEOUT=15'
  check:
    comment: 'In out-of-band contexts, enable all checks'
    nodes:   '*'
    command: 'export NHC_CHECK_ALL=1'
  verbose:
    comment: 'Run verbosely'
    nodes:   '*'
    command: 'export VERBOSE=1'
  eth0:
    comment: 'Checks for an active ethernet interface named "eth0."'
    nodes:   '*'
    command: 'check_hw_eth eth0'
  mcelog:
    comment: 'Check the mcelog daemon for any pending errors'
    nodes:   '*'
    command: 'check_hw_mcelog'
  rootrw:
    comment: 'All nodes should have their root filesystem mounted read/write'
    nodes:   '*'
    command: 'check_fs_mount_rw -f /'
  rootfree:
    comment: "Make sure the root filesystem doesn't get too full"
    nodes:   '*'
    command: 'check_fs_free / 3%'
  tmpfree:
    comment: "Make sure the /tmp filesystem doesn't get too full"
    nodes:   '*'
    command: 'check_fs_free /tmp 3%'
  home-stat:
    comment: 'Assert that /home filesystem is available'
    nodes:   '*'
    command: "check_file_test -d %{hiera('home_dir')}"
  homefree:
    comment: "Make sure the /home filesystem doesn't get too full"
    nodes:   '*'
    command: "check_fs_free %{hiera('home_dir')} 3%"
  tmprw:
    comment: 'These should always be directories and always be read/write/execute and sticky'
    nodes:   '*'
    command: 'check_file_test -r -w -x -d -k /tmp /var/tmp'
  devnull:
    comment: 'Assert common properties for /dev/null (which occasionally gets clobbered)'
    nodes:   '*'
    command: 'check_file_test -c -r -w /dev/null /dev/zero'

##### CLARA
# repos - repos.ini
profiles::clara::dist_mirror_source_server: "ftp.scibian.org"
profiles::clara::hpc_mirror_source_server:  "ftp.scibian.org"
profiles::clara::base_repos_root:           "%{hiera('website_dir')}/mirror"
profiles::clara::repos:
  #Suites Calibre 9
  'jessie':
    suite_name:  'jessie'
    mirror_dir:  'calibre9/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
    sections:    'main,contrib,non-free,main/debian-installer'
  'jessie-updates':
    suite_name:  'jessie-updates'
    mirror_dir:  'calibre9/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'jessie-security':
    suite_name:  'jessie/updates'
    mirror_dir:  'calibre9/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'jessie-backports':
    suite_name:  'jessie-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre9':
    suite_name:  'calibre9'
    mirror_dir:  'calibre9/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre9-hpc':
    suite_name:  'calibre9'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
    archs:       'amd64'
  #Suites Calibre 8
  'wheezy':
    suite_name:  'wheezy'
    mirror_dir:  'calibre8/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
    sections:    'main,contrib,non-free,main/debian-installer'
  'wheezy-updates':
    suite_name:  'wheezy-updates'
    mirror_dir:  'calibre8/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'wheezy-security':
    suite_name:  'wheezy/updates'
    mirror_dir:  'calibre8/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'wheezy-backports':
    suite_name:  'wheezy-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre8':
    suite_name:  'calibre8'
    mirror_dir:  'calibre8/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'calibre8-hpc':
    suite_name:  'calibre8'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
    archs:       'amd64'
  #Suites Calibre 7
  'squeeze':
    suite_name:  'squeeze'
    mirror_dir:  'calibre7/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'
    sections:    'main,contrib,non-free,main/debian-installer'
  'squeeze-security':
    suite_name:  'squeeze/updates'
    mirror_dir:  'calibre7/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'
  'squeeze-backports':
    suite_name:  'squeeze-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7':
    suite_name:  'calibre7'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp2':
    suite_name:  'calibre7-sp2'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp4':
    suite_name:  'calibre7-sp4'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp5':
    suite_name:  'calibre7-sp5'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-hpc':
    suite_name:  'calibre7'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'
clara::common_options:
  origin:      "%{hiera('cluster_repo_originator')}"
clara::repo_options:
  gpg_key:     "%{hiera('cluster_repo_gpg_key')}"
  clustername: "%{hiera('cluster_name')}"
  mirror_root: "%{hiera('profiles::http::mirror::docroot')}"
clara::ipmi_options:
  conmand:     "%{hiera('cluster_prefix')}%{my_conman_server}"
clara::keyring_source: "%{hiera('private_files_dir')}/repo/cluster_keyring.secret.gpg.enc"
clara::password_options:
  ASUPASSWD:            "%{hiera('cluster_decrypt_password')}"

##### Ftp
proftpd::user_home: "%{hiera('admin_dir')}/restricted/ftp"

##### APT config
apt::always_apt_update:    true
apt::disable_keys:         false
apt::purge_sources_list:   true
apt::purge_sources_list_d: true
apt::purge_preferences_d:  true
apt::update_timeout:       '10'
apt::proxy_port:           3142

##### Slurmweb
slurmweb::decrypt_passwd: "%{hiera('cluster_decrypt_password')}"
profiles::http::slurmweb::config_options:
  infos:
    version:
      comment: ''
      value:   '2.0'
  cors:
    authorized_origins:
      comment: ''
      value:   'https://your_dashboard'
  config:
    server_name:
      comment: ''
      value:   "%(hostname)"
    secret_key:
      comment: ''
      value:   'none'
    authentication:
      comment: ''
      value:   'disable'
    cache:
      comment: ''
      value:   'disable'
    racksxml:
      comment: ''
      value:   '/etc/slurm-web/racks.xml'
  roles:
    all:
      comment: ''
      value:   'all'
    user:
      comment: ''
      value:   "@%{hiera('user_group')}"
    admin:
      comment: ''
      value:   "@%{hiera('admin_group')}"
    restricted_fields_for_all:
      comment: ''
      value:   'command'
    restricted_fields_for_user:
      comment: ''
      value:   'command'
    restricted_fields_for_admin:
      comment: ''
      value:   ''
  acl:
    3dview:
      comment: ''
      value:   "@%{hiera('admin_group')}"

##### Logs
profiles::log::client::remote_type:   'repl'
profiles::log::client::log_local:     true
profiles::log::client::server:        "%{hiera('cluster_prefix')}%{::my_log_server}"
profiles::log::client::port:          '20514'
profiles::log::server::server_dir:    "%{hiera('admin_dir')}/restricted/logs/"
profiles::log::server::custom_config: 'rsyslog/server-hostname.conf.erb'
