###### Global Organization settings ######
# These  values should be redefined for your site
cluster:                  'mycluster'
cluster_prefix:           'my'
domain:                   'hpc.example.com'
cluster_repo_originator:  'TeamHPC'
cluster_repo_gpg_key:     '01234567'
cluster_decrypt_password: 'password'
# These values might be redefined for your site
admin_dir:      '/admin'
cluster_name:   "%{hiera('cluster')}"
website_dir:    "%{hiera('admin_dir')}/public/http"
preseeddir:     '/var/cache/debconf'
libcalibre:     '/var/lib/calibre'

###### NTP
ntp::iburst_enable: false
ntp::disable_auth: true
ntp::disable_monitor: false
ntp::driftfile: '/var/lib/ntp/ntp.drift'
ntp::restrict: 
    - '-4 default kod notrap nomodify nopeer'
    - '-6 default kod notrap nomodify nopeer'
    - "-4 %{hiera('net::allloc::ipnetwork')} mask %{hiera('net::allloc::netmask')} notrap nomodify"
    - '127.0.0.1'
    - '::1' 

profiles::ntp::srv_opts:
    NTPD_OPTS: "'-4 -g'"

###### SLURM
slurm_primary_server:      'batch1'
slurm_secondary_server:    'batch2'
slurm_user:                'slurm'
slurm_cluster_name:        "%{hiera('cluster_name')}"
slurmd_user:               'root'

slurmd::enable_cgroup: true

slurmcommons::slurm_conf_options:
    ClusterName: 
      value:   "%{hiera('slurm_cluster_name')}"
      comment: "The name by which this SLURM managed cluster is known in the accounting database"
    ControlMachine:
      value:   "%{hiera('slurm_primary_server')}"
      comment: "Hostname of the machine where SLURM control functions are executed"
    BackupController:
      value:   "%{hiera('slurm_secondary_server')}"
      comment: "Machine where SLURM control functions are to be executed if ControlMachine fails"
    SlurmUser:
      value:   "%{hiera('slurm_user')}"
      comment: "The name of the user that the slurmctld daemon executes as"
    SlurmdUser:
      value:   "%{hiera('slurmd_user')}"
      comment: "The name of the user that the slurmd daemon executes as"
    SlurmctldPort:
      value:   "6817"
      comment: "The port number that the SLURM controller slurmctld listens to for work"
    SlurmdPort:
      value:   "6818"
      comment: "The port number that the SLURM compute node daemon slurmd listens to for work. SlurmctldPort and SlurmdPort must be different"
    SlurmctldPidFile:
      value:   "/var/run/slurm-llnl/slurmctld.pid"
      comment: "File into which the slurmctld daemon may write its process id"
    SlurmdPidFile:
      value:   "/var/run/slurm-llnl/slurmd.pid"
      comment: "File into which the slurmd daemon may write its process id"
    SlurmdSpoolDir:
      value:   "/var/spool/slurmd"
      comment: "Directory (local file system) into which the slurmd daemons state information and batch job script information are written"
    AuthType:
      value:   "auth/munge"
      comment: "The authentication method for communications between SLURM components"
    CryptoType:
      value:   "crypto/munge"
      comment: "The cryptographic signature tool to be used in the creation of job step credentials"
    DisableRootJobs:
      value:   "YES"
      comment: "If set to 'YES' then user root will be prevented from running any jobs"
    CacheGroups:
      value:   "0"
      comment: "If set to 1 the slurmd daemon will cache /etc/groups entries. This can improve performance for highly parallel jobs"
    GroupUpdateForce:
      value:   "1"
      comment: "If set to 1 then information about which users are members of groups allowed to use a partition will be updated periodically"
    GroupUpdateTime:
      value:   "600"
      comment: "Controls how frequently information about which users are members of groups allowed to use a partition will be updated"
    UsePAM:
      value:   "1"
      comment: "If set to 1 authentication by PAM will be enabled"
    SwitchType:
      value:   "switch/none"
      comment: "Identifies the type of switch or interconnect used for application communications. Acceptable values include 'switch/none' and 'switch/elan'"
    TreeWidth:
      value:   "600"
      comment: "Slurmd daemons use a virtual tree network for communications. IMPORTANT: configure TreeWidth to a number at least as large as the maximum node count!!"
    Prolog:
      value:   "/usr/lib/slurm/generic-scripts/Prolog.sh"
      comment: "Script executed at job step initiation on that node"
    PrologSlurmctld:
      value:   "/usr/lib/slurm/generic-scripts/PrologSlurmctld.sh"
      comment: "Script executed at job allocation"
    TaskProlog:
      value:   "/usr/lib/slurm/generic-scripts/TaskProlog.sh"
      comment: "Script executed at job step initiation by user invoking srun command"
    SrunProlog:
      value:   "/usr/lib/slurm/generic-scripts/SrunProlog.sh"
      comment: "Script executed at job step initiation by user invoking sbatch command"
    Epilog:
      value:   "/usr/lib/slurm/generic-scripts/Epilog.sh"
      comment: "Script executed at job termination by %{hiera('slurmd_user')}"
    EpilogSlurmctld:
      value:   "/usr/lib/slurm/generic-scripts/EpilogSlurmctld.sh"
      comment: "Script executed at job termination by %{hiera('slurm_user')}"
    TaskEpilog:
      value:   "/usr/lib/slurm/generic-scripts/TaskEpilog.sh"
      comment: "Script executed at completion job step by user invoking sbatch command"
    SrunEpilog:
      value:   "/usr/lib/slurm/generic-scripts/SrunEpilog.sh"
      comment: "Script executed at completion job step by user invoking srun command"
    HealthCheckProgram:
      value:   "/usr/sbin/nhc"
      comment: "Script to execute as user root periodically on all compute nodes. This may be used to verify the node is fully operational and DRAIN the node if a problem is detected."
    HealthCheckInterval:
     value:   "300"
     comment: "The interval in seconds between executions of HealthCheckProgram. If value is zero execution will be disabled"
    MailProg:
      value:   "/usr/bin/mail"
      comment: "Program used to send email per user request"
    BatchStartTimeout:
      value:   "10"
      comment: "The maximum time (in seconds) that a batch job is permitted for launching before being considered missing and releasing the allocation"
    CompleteWait:
      value:   "0"
      comment: "The time in seconds given for a job to remain in COMPLETING state before any additional jobs are scheduled"
    KillWait:
     value:   "30"
     comment: "The interval in seconds given to a job's processes between the SIGTERM and SIGKILL signals upon reaching its time limit"
    EpilogMsgTime:
     value:   "2000"
     comment: "The number of microseconds that the slurmctld daemon requires to process an epilog completion message from the slurmd dameons"
    HealthCheckInterval:
      value:   "300"
      comment: "The interval in seconds between executions of HealthCheckProgram. If value is zero execution will be disabled"
    InactiveLimit:
      value:   "0"
      comment: "IMPORTANT : Script /usr/local/sbin/slurm_srun_prolog.visu require set up this value to zero"
    MessageTimeout:
      value:   "30"
      comment: "Time permitted for a round-trip communication to complete in seconds"
    ResvOverRun:
      value:   "10"
      comment: "Describes how long (in minutes] a job already running in a reservation should be permitted to execute after the end time of the reservation has been reached"
    MinJobAge:
      value:   "300"
      comment: "The minimum age of a completed job before its record is purged from SLURM's active database"
    MaxJobCount:
      value:   "5000"
      comment: "The maximum number of jobs SLURM can have in its active database at one time"
    FirstJobId:
      value:   "1"
      comment: "The job id to be used for the first submitted to SLURM without a specific requested value"
    MaxJobId:
      value:   "2147450880"
      comment: "The maximum job id to be used for jobs submitted to SLURM without a specific requested value"
    OverTimeLimit:
      value:   "3"
      comment: "Number of minutes by which a job can exceed its time limit before being canceled"
    SlurmctldTimeout:
      value:   "60"
      comment: "The interval in seconds that the backup controller waits for the primary controller to respond before assuming control"
    SlurmdTimeout:
      value:   "120"
      comment: "The interval in seconds that the SLURM controller waits for slurmd to respond before configuring that node's state to DOWN"
    UnkillableStepTimeout:
      value:   "60"
      comment: "The length of time in seconds that SLURM will wait before deciding that processes in a job step are unkillable (after they have been signaled with SIGKILL)"
    vSizeFactor:
      value:   "0"
      comment: "The job will be killed if its real memory exceeds 500MB or its virtual memory exceeds 505MB. If set to 0 this feature is disabled."
    waittime:
      value:   "0"
      comment: "Specifies how many seconds the srun command should by default wait after the first task terminates before terminating all remaining tasks. If set to 0 this feature is disabled."
    DefMemPerCPU:
      value:   "0"
      comment: "Default real memory size available per allocated CPU in MegaBytes. 0 == unlimited."
    MaxMemPerCPU:
      value:   "0"
      comment: "Maximum real memory size available per allocated CPU in MegaBytes. 0 == unlimited."
    FastSchedule:
      value:   "1"
      comment: "TO REVIEW"
    SchedulerRootFilter:
      value:   "1"
      comment: "If set to 0 then RootOnly partitions are treated like any other partition. If set to 1 then RootOnly partitions are exempt from any external scheduling activities"
    SchedulerTimeSlice:
      value:   "30"
      comment: "Number of seconds in each time slice when gang scheduling is enabled (PreemptMode=GANG)"
    SchedulerType:
      value:   "sched/backfill"
      comment: "sched/wiki or sched/wiki2: for the Wiki interface to the Maui Schedule"
    SelectType:
      value:   "select/cons_res"
      comment: "select/cons_res: The resources within a node are individually allocated as consumable resources. Note that whole nodes can be allocated to jobs for selected partitions by using the Shared=Exclusive option. Defines specific subsystems which should provide more detailed event logging. Valid subsystems available today (with more to come) include:"
    SelectTypeParameters:
      value:   "CR_CPU_Memory"
      comment: ""
    PropagatePrioProcess:
      value:   "0"
      comment: "2: Like 1 with the restriction that their nice value will always be one higher than the slurm daemon"
    PropagateResourceLimits:
      value:   "NONE"
      comment: "The following limit names are supported by SLURM: ALL NONE AS CORE CPU DATA FSIZE MEMLOCK NOFILE NPROC RSS STACK. A list of comma separated resource limit names to exclude"
    CheckpointType:
      value:   "checkpoint/none"
      comment: "Possibles values for GNU/Linux are checkpoint/blcr; checkpoint/ompi; checkpoint/none "
    StateSaveLocation:
      value:   "/var/lib/slurm-llnl/slurmctld"
      comment: "Directory into which the SLURM controller (slurmctld) saves its state"
    JobRequeue:
      value:   "1"
      comment: ""
    EnforcePartLimits:
      value:   "YES"
      comment: "If set to 'YES' then jobs which exceed a partition's size and/or time limits will be rejected at submission time"
    JobSubmitPlugins:
      value:   "lua"
      comment: "A comma delimited list of job submission plugins to be used. The specified plugins will be executed in the order listed."
    MpiDefault:
      value:   "none"
      comment: "Identifies the default type of MPI to be used. Currently supported versions include: lam mpich1_p4 mpich1_shmem mpichgm mpichmx mvapich none and openmpi"
    TmpFs:
      value:   "/tmp"
      comment: "Pathname of the file system available to user jobs for temporary storage"
    JobFileAppend:
      value:   "0"
      comment: ""
    KillOnBadExit:
      value:   "0"
      comment: "Users can override this configuration parameter by using srun -K --kill-on-bad-exit."
    MaxStepCount:
      value:   "100000"
      comment: "The maximum number of steps that any job can initiate"
    MaxTasksPerNode:
      value:   "28"
      comment: "Maximum number of tasks SLURM will allow a job step to spawn on a single node"
    TaskPlugin:
      value:   "task/cgroup"
      comment: "'task/affinity' 'task/cgroup' or 'task/none'"
    ProctrackType:
      value:   "proctrack/cgroup"
      comment: "Identifies the plugin to be used for process tracking"
    PriorityType:
      value:   "priority/basic"
      comment: "This specifies the plugin to be used in establishing a jobs scheduling priority. Supported values are 'priority/basic' and 'priority/multifactor'"
    AccountingStorageType:
      value:   "accounting_storage/slurmdbd"
      comment: "'accounting_storage/filetxt' 'accounting_storage/mysql' 'accounting_storage/none' 'accounting_storage/pgsql' and 'accounting_storage/slurmdbd'"
    AccountingStorageHost:
      value:   "%{hiera('slurm_primary_server')}"
    AccountingStorageBackupHost:
      value:   "%{hiera('slurm_secondary_server')}"
    TrackWCKey:
      value:   "yes"
      comment: "If you want to track wckeys (Workload Characterization Key) of users."
    JobCompType:
      value:   "jobcomp/none"
      comment: "The job completion logging mechanism type. Acceptable values at present include 'jobcomp/none' 'jobcomp/filetxt' 'jobcomp/mysql' 'jobcomp/pgsql' and 'jobcomp/script'"
    JobAcctGatherType:
      value:   "jobacct_gather/linux"
      comment: "The job accounting mechanism type. Acceptable values at present include 'jobacct_gather/aix' 'jobacct_gather/linux' and 'jobacct_gather/none'"
    JobAcctGatherFrequency:
      value:   "60"
      comment: "A value of zero disables real the periodic job sampling and provides accounting information only on job termination. "
    SlurmctldDebug:
      value:   "3"
      comment: "The level of detail to provide slurmctld daemon's logs. Values from 0 to 9 are legal"
    SlurmdDebug:
      value:   "3"
      comment: "The level of detail to provide slurmd daemon's logs. Values from 0 to 9 are legal"
    SlurmctldLogFile:
      value:   "/var/log/slurm-llnl/slurmctld.log"
      comment: "Log file for slurmctld daemon"
    SlurmdLogFile:
      value:   "/var/log/slurm-llnl/slurmd.log"
      comment: "Log file for slurmd daemon"
    ReturnToService:
      value:   "2"
      comment: "Controls when a DOWN node will be returned to service. 1 = A DOWN node will become available for use upon registration with a valid configuration only if it was set DOWN due to being non-responsive"
    SuspendTime:
      value:   "-1"
      comment: "Nodes becomes eligible for power saving mode after being idle for this number of seconds"
    Include:
      value:   "/etc/slurm-llnl/partitions.conf"
      comment: "If a line begins with the word 'Include' followed by whitespace and then a file name, that file will be included inline with the current configuration file"


##### CLARA
# repos - repos.ini
profiles::clara::dist_mirror_source_server: "ftp.scibian.org"
profiles::clara::hpc_mirror_source_server:  "ftp.scibian.org"
profiles::clara::base_repos_root:           "%{hiera('website_dir')}/mirror"
profiles::clara::repos:
  #Suites Calibre 9
  'jessie':
    suite_name:  'jessie'
    mirror_dir:  'calibre9/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
    sections:    'main,contrib,non-free,main/debian-installer'
  'jessie-updates':
    suite_name:  'jessie-updates'
    mirror_dir:  'calibre9/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'jessie-security':
    suite_name:  'jessie/updates'
    mirror_dir:  'calibre9/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'jessie-backports':
    suite_name:  'jessie-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre9':
    suite_name:  'calibre9'
    mirror_dir:  'calibre9/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre9-hpc':
    suite_name:  'calibre9'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
    archs:       'amd64'
  #Suites Calibre 8
  'wheezy':
    suite_name:  'wheezy'
    mirror_dir:  'calibre8/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
    sections:    'main,contrib,non-free,main/debian-installer'
  'wheezy-updates':
    suite_name:  'wheezy-updates'
    mirror_dir:  'calibre8/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'wheezy-security':
    suite_name:  'wheezy/updates'
    mirror_dir:  'calibre8/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'wheezy-backports':
    suite_name:  'wheezy-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre9/"
  'calibre8':
    suite_name:  'calibre8'
    mirror_dir:  'calibre8/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
  'calibre8-hpc':
    suite_name:  'calibre8'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre8/"
    archs:       'amd64'
  #Suites Calibre 7
  'squeeze':
    suite_name:  'squeeze'
    mirror_dir:  'calibre7/debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'
    sections:    'main,contrib,non-free,main/debian-installer'
  'squeeze-security':
    suite_name:  'squeeze/updates'
    mirror_dir:  'calibre7/debian-security'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'
  'squeeze-backports':
    suite_name:  'squeeze-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7':
    suite_name:  'calibre7'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp2':
    suite_name:  'calibre7-sp2'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp4':
    suite_name:  'calibre7-sp4'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-sp5':
    suite_name:  'calibre7-sp5'
    mirror_dir:  'calibre7/calibre'
    server:      "%{hiera('profiles::clara::dist_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
  'calibre7-hpc':
    suite_name:  'calibre7'
    mirror_dir:  'calibre-hpc'
    server:      "%{hiera('profiles::clara::hpc_mirror_source_server')}"
    mirror_root: "%{hiera('profiles::clara::base_repos_root')}/calibre7/"
    archs:       'amd64'

##### CLARA
clara::common_options:
  origin:      "%{hiera('cluster_repo_originator')}"
clara::repo_options:
  gpg_key:     "%{hiera('cluster_repo_gpg_key')}"
  clustername: "%{hiera('cluster_name')}"
clara::ipmi_options:
  conmand:     "%{hiera('cluster_prefix')}%{my_conman_server}" 
