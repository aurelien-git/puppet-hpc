###### Global Organization settings ######
# These  values should be redefined for your site
cluster_name:             'mycluster'
cluster_prefix:           'my'
domain:                   'hpc.example.com'
virtual_domain:           'virtual'
cluster_decrypt_password: 'password'
# These values might be redefined for your site
admin_dir:      '/var/admin'
website_dir:    '/var/www'
preseeddir:     '/var/cache/debconf'
libcalibre:     '/var/lib/calibre'
home_dir:       '/home'

###### NTP
ntp::iburst_enable: false
ntp::disable_auth: true
ntp::disable_monitor: false
ntp::driftfile: '/var/lib/ntp/ntp.drift'
# IPv6 is disabled in /etc/default, so we don't include
# localhost IPv6 addresses in restrict
ntp::restrict: 
  - 'default kod notrap nomodify nopeer'
  - "%{hiera('net::administration::ipnetwork')} mask %{hiera('net::administration::netmask')} notrap nomodify"
  - "%{hiera('net::management::ipnetwork')} mask %{hiera('net::management::netmask')} notrap nomodify"
  - '127.0.0.1'
profiles::ntp::srv_opts:
  NTPD_OPTS: "'-4 -g'"
profiles::ntp::client::site_preferred_servers:
  - "%{hiera('cluster_prefix')}%{::my_ntp_server}1"
profiles::ntp::client::site_servers:
  - "%{hiera('cluster_prefix')}%{::my_ntp_server}2"

##### DNS
dns::server::virtual_domain: "%{hiera('virtual_domain')}"
install_server_ip: "%{::ipaddress}" # May be required to change with the IP
                                    # address of the first installation server
                                    # on the administration network.
# The virtual zone is configured with the following entries in dns server only
# if dns::server::virtual_relay is false (default: true). This is useful on the
# early stage on a cluster installation process when the services depends on the
# virtual zone and consul cluster has not enough members to be bootstrapped yet.
dns::server::virtual_entries:
 - owner: '@'
   proto: 'IN'
   type:  'NS'
   data:  "%{::fqdn}."
 - owner: 'apt.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"
 - owner: 'web-boot.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"
 - owner: 'secret.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"
 - owner: 'http.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"
 - owner: 'rgw.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"
 - owner: 's3-system.service'
   proto: 'IN'
   type:  'A'
   data:  "%{hiera('install_server_ip')}"

dns::client::options:
  - rotate
  - timeout:1


##### DHCP

iscdhcp::bootmenu_url: "http://%{hiera('server_web_boot')}/cgi-bin/bootmenu.py?node=${hostname}"
# This array contains iPXE specific options for ISC DHCP server. They must be
# defined in DHCP server configuration in order to be used then in iPXE menus
# and scripts. Please refer to iPXE documentation for reference:
#   http://ipxe.org/howto/dhcpd#ipxe-specific_options
profiles::dhcp::global_options:
  - 'dhcp-max-message-size 2048'
  - 'space ipxe'
  - 'ipxe-encap-opts code 175 = encapsulate ipxe'
  - 'ipxe.priority code 1 = signed integer 8'
  - 'ipxe.keep-san code 8 = unsigned integer 8'
  - 'ipxe.skip-san-boot code 9 = unsigned integer 8'
  - 'ipxe.syslogs code 85 = string'
  - 'ipxe.cert code 91 = string'
  - 'ipxe.privkey code 92 = string'
  - 'ipxe.crosscert code 93 = string'
  - 'ipxe.no-pxedhcp code 176 = unsigned integer 8'
  - 'ipxe.bus-id code 177 = string'
  - 'ipxe.bios-drive code 189 = unsigned integer 8'
  - 'ipxe.username code 190 = string'
  - 'ipxe.password code 191 = string'
  - 'ipxe.reverse-username code 192 = string'
  - 'ipxe.reverse-password code 193 = string'
  - 'ipxe.version code 235 = string'
  - 'iscsi-initiator-iqn code 203 = string'
  - 'ipxe.pxeext code 16 = unsigned integer 8'
  - 'ipxe.iscsi code 17 = unsigned integer 8'
  - 'ipxe.aoe code 18 = unsigned integer 8'
  - 'ipxe.http code 19 = unsigned integer 8'
  - 'ipxe.https code 20 = unsigned integer 8'
  - 'ipxe.tftp code 21 = unsigned integer 8'
  - 'ipxe.ftp code 22 = unsigned integer 8'
  - 'ipxe.dns code 23 = unsigned integer 8'
  - 'ipxe.bzimage code 24 = unsigned integer 8'
  - 'ipxe.multiboot code 25 = unsigned integer 8'
  - 'ipxe.slam code 26 = unsigned integer 8'
  - 'ipxe.srp code 27 = unsigned integer 8'
  - 'ipxe.nbi code 32 = unsigned integer 8'
  - 'ipxe.pxe code 33 = unsigned integer 8'
  - 'ipxe.elf code 34 = unsigned integer 8'
  - 'ipxe.comboot code 35 = unsigned integer 8'
  - 'ipxe.efi code 36 = unsigned integer 8'
  - 'ipxe.fcoe code 37 = unsigned integer 8'
  - 'ipxe.vlan code 38 = unsigned integer 8'
  - 'ipxe.menu code 39 = unsigned integer 8'
  - 'ipxe.sdi code 40 = unsigned integer 8'
  - 'ipxe.nfs code 41 = unsigned integer 8'
  - 'ipxe.no-pxedhcp 1'
  - 'arch code 93 = unsigned integer 16'


##### SSMTP
profiles::ssmtp::config_options:
  mailhub:       "%{hiera('cluster_prefix')}%{::my_postfix_relay}"
  rewritedomain: "%{::hostname}.%{hiera('domain')}"

##### POSTFIX
profiles::postfix::relay::config_options:
  myhostname:          "%{hostname}"
  mydomain:            "%{hiera('domain')}"

##### TFTP
# This shared parameter is not used in this layer of the hieradata but it is
# proven to be useful in upper layers.
tftp_dir: '/srv/tftp'

##### SSH
profiles::openssh::server::config_augeas:
  - 'set MaxStartups 8192'
  - 'set PermitRootLogin yes'
  - 'set X11UseLocalhost no'
  - 'set ClientAliveInterval 300'
  - 'set KexAlgorithms/1 curve25519-sha256@libssh.org'
  - 'set KexAlgorithms/2 ecdh-sha2-nistp256'
  - 'set KexAlgorithms/3 ecdh-sha2-nistp384'
  - 'set KexAlgorithms/4 ecdh-sha2-nistp521'
  - 'set KexAlgorithms/5 diffie-hellman-group-exchange-sha256'
  - 'set KexAlgorithms/6 diffie-hellman-group14-sha1'
  - 'set KexAlgorithms/7 diffie-hellman-group-exchange-sha1'
  - 'set KexAlgorithms/8 diffie-hellman-group1-sha1'
  - 'set AcceptEnv/1 LANG'
  - 'set AcceptEnv/2 LC_*'
  - 'set AcceptEnv/3 SLURM_*'
  - 'set AcceptEnv/4 SCRATCHDIR'

openssh::server::hostkeys_source_dir: "%{hiera('private_files_dir')}/hostkeys"
openssh::server::decrypt_passwd:      "%{hiera('cluster_decrypt_password')}"

profiles::openssh::client::config_augeas:
  - 'set StrictHostKeyChecking no'
  # If a users connects with GSS (kerberos) credentials will be passed on
  # the session. This is usefull when connecting from a frontend node to 
  # a compute or graphical node.
  - 'set GSSAPIDelegateCredentials yes'
  - 'set GSSAPIRenewalForcesRekey yes'
  - 'set SendEnv/1 LANG'
  - 'set SendEnv/2 LC_*'
  - 'set SendEnv/3 SLURM_*'
  - 'set SendEnv/4 SCRATCHDIR'

profiles::openssh::client::identities:
  '/root/.ssh/id_rsa_root':
    key_enc:        "%{hiera('private_files_dir')}/rootkeys/id_rsa_root.enc"
    config_file:    '/root/.ssh/config'
    host:           '*'
    decrypt_passwd: "%{hiera('cluster_decrypt_password')}"

##### Codes packages
codes::packages:
  - "%{hiera('cluster_name')}-hpc-%{puppet_role}"
## Values below should be overriden by roles
profiles::access::production_options: []
profiles::access::maintenance_options: []

##### Access Rules #####
profiles::access::maintenance_mode: false

##### SSSD
profiles::auth::client::enable_kerberos: false
profiles::auth::client::sssd_options_general:
  config_file_version:  '2'
  reconnection_retries: '3'
  sbus_timeout:         '30'
  services:             'nss,pam'
  domains:              "%{hiera('cluster_name')}"
profiles::auth::client::sssd_options_nss:
  filter_groups:        'root'
  filter_users:         'root'
  reconnection_retries: '3'
profiles::auth::client::sssd_options_pam:
  reconnection_retries:           '3'
  offline_credentials_expiration: '1'
profiles::auth::client::krb5_server:                    "%{hiera('kdc_primary')}, %{hiera('kdc_secondary')}"
profiles::auth::client::krb5_realm:                     "%{hiera('krb5_realm')}"
profiles::auth::client::krb5_store_password_if_offline: 'true'
kerberos::keytab_source_dir:                            "%{hiera('private_files_dir')}/auth/keytabs"
kerberos::decrypt_passwd:                               "%{hiera('cluster_decrypt_password')}"

###### Openldap
profiles::auth::replica::ldif_directory:   "%{hiera('libcalibre')}/slapdcfg"
profiles::auth::replica::ldif_file:        'config_replica.ldif'
profiles::auth::replica::directory_source: "%{hiera('private_files_dir')}/auth"
certificates::directory_source:            "%{hiera('private_files_dir')}/auth"
certificates::certificates_directory:      '/etc/ldap/certificates'
certificates::certificate_file:            "%{hiera('cluster_name')}_ldap.crt"
certificates::key_file:                    "%{hiera('cluster_name')}_ldap.key"
certificates::decrypt_passwd:              "%{hiera('cluster_decrypt_password')}"
openldap::replica::decrypt_passwd:         "%{hiera('cluster_decrypt_password')}"

##### Sudo #####

# basic sudo config lines
profiles::sudo::sudo_config_opts:
  - 'root  ALL =  (ALL) ALL'
  - '%sudo  ALL =  (ALL) ALL'
  - '#includedir /etc/sudoers.d'

##### HTTP
apache::apache_version:          '2.4'
profiles::http::port:            80
profiles::http::error_log_file:  'error.log'
profiles::http::log_level:       'warn'
profiles::http::serveradmin:     'webmaster@localhost'
profiles::http::scriptalias:     "%{hiera('profiles::http::mirror::docroot')}/cgi-bin"
profiles::http::mirror::docroot: "%{hiera('website_dir')}/mirror"

###### EYAML keys propagation service
secret_port:                           '1216'
profiles::http::secret::port:          "%{hiera('secret_port')}"
profiles::http::secret::docroot:       '/var/www_secret'
profiles::http::secret::keys_enc:      "%{hiera('private_files_dir')}/eyaml/keys.tar.xz.enc"
profiles::http::secret::keys_password: "%{hiera('cluster_decrypt_password')}"

profiles::http::diskless::port:    '3137'
profiles::http::diskless::docroot: "%{hiera('website_dir')}/diskless"

##### BOOTSYSTEM
boot_http_port:            '3138'
# HTTP base urls for boots
server_web_boot:           "web-boot.service.%{hiera('virtual_domain')}:%{hiera('boot_http_port')}"
boothttp::port:            "%{hiera('boot_http_port')}"
boothttp::config_dir_http: "%{hiera('website_dir')}"
boothttp::menu_source:     "%{hiera('private_files_dir')}/boot/cgi/bootmenu.py"

##### HA

# The keepalived public module wrongly set these service params to false in its
# params.pp. As a workaround to this bug, we set the params with hiera
# autolookups.
keepalived::service_hasrestart: true
keepalived::service_hasstatus: true

##### P2P
profiles::p2p::tracker_path: "%{hiera('profiles::http::diskless::docroot')}"

##### Munge
munge::auth_key_source: "%{hiera('private_files_dir')}/munge/munge.key.enc"
munge::decrypt_passwd:  "%{hiera('cluster_decrypt_password')}"

###### SLURM
slurm_primary_server:      "%{hiera('cluster_prefix')}%{::my_jobsched_server}1"
slurm_secondary_server:    "%{hiera('cluster_prefix')}%{::my_jobsched_server}2"
slurm_state_save_loc:      "/var/spool/slurmctld"
slurm::exec::enable_cgroup: true
profiles::jobsched::slurm_config_options:
  ClusterName:                 "%{hiera('cluster_name')}"
  ControlMachine:              "%{hiera('slurm_primary_server')}"
  BackupController:            "%{hiera('slurm_secondary_server')}"
  DisableRootJobs:             "YES"
  GroupUpdateForce:            "1"
  UsePAM:                      "1"
  TreeWidth:                   "600"
  HealthCheckProgram:          "/usr/sbin/nhc"
  HealthCheckInterval:         "300"
  ResvOverRun:                 "10"
  MaxJobCount:                 "5000"
  SlurmctldTimeout:            "60"
  SlurmdTimeout:               "120"
  SelectType:                  "select/cons_res"
  SelectTypeParameters:        "CR_CPU_Memory"
  StateSaveLocation:           "%{hiera('slurm_state_save_loc')}"
  PropagateResourceLimits:     "NONE"
  EnforcePartLimits:           "YES"
  JobSubmitPlugins:            "lua"
  MaxStepCount:                "100000"
  MaxTasksPerNode:             "28"
  TaskPlugin:                  "task/cgroup"
  ProctrackType:               "proctrack/cgroup"
  PriorityType:                "priority/multifactor"
  AccountingStorageType:       "accounting_storage/slurmdbd"
  AccountingStorageHost:       "%{hiera('slurm_primary_server')}"
  AccountingStorageBackupHost: "%{hiera('slurm_secondary_server')}"
  TrackWCKey:                  "yes"
  JobAcctGatherType:           "jobacct_gather/linux"
  JobAcctGatherFrequency:      "60"
  ReturnToService:             "2"
  GresTypes:                   "gpu"

##### Slurm DBD database
db_primary_server:      "%{hiera('cluster_prefix')}%{::my_db_server}1"
db_secondary_server:    "%{hiera('cluster_prefix')}%{::my_db_server}2"
mariadb::nodes:
 - "%{hiera('db_primary_server')}"
 - "%{hiera('db_secondary_server')}"
profiles::jobsched::server::slurmdbd_config_options:
  DbdHost:           "%{hiera('slurm_primary_server')}"
  DbdBackupHost:     "%{hiera('slurm_secondary_server')}"
  StorageHost:       "%{hiera('db_primary_server')}"
  StorageBackupHost: "%{hiera('db_secondary_server')}"
  StoragePass:       "%{hiera('slurmdbd_slurm_db_password')}"
slurm::dbd::db_options:
  db:
    password:    "%{hiera('mariadb::mysql_root_pwd')}"
  passwords:
    slurm:       "%{hiera('slurmdbd_slurm_db_password')}"
    slurmro:     "%{hiera('slurmdbd_slurmro_db_password')}"
  hosts:
    controllers: "%{hiera('slurm_primary_server')}.%{hiera('domain')},%{hiera('slurm_secondary_server')}.%{hiera('domain')}"
    admins:      "%{hiera('cluster_prefix')}admin1.%{hiera('domain')}"


profiles::jobsched::server::sync_options:
  main:
    cluster: "%{hiera('cluster_name')}"
    group:   "%{hiera('user_group')}"

profiles::jobsched::server::sync_cron_hour_primary: 2
profiles::jobsched::server::sync_cron_hour_secondary: 14

##### NHC
profiles::warewulf_nhc::config_options:
  10_verbose:
    comment: 'Run verbosely'
    nodes:   '*'
    command: 'export VERBOSE=1'
  10_debug:
    comment: 'Activate debugging mode'
    nodes:   '*'
    command: 'export DEBUG=0'
  10_timestamp:
    comment: 'Activate timestamps in logs'
    nodes:   '*'
    command: 'export TS=1'
  20_watchdog:
    comment: 'Set watchdog timer to 15 seconds'
    nodes:   '*'
    command: 'export TIMEOUT=15'
  20_rm:
    comment: 'Explicitly instruct NHC to assume the Resource Manager'
    nodes:   '*'
    command: 'export NHC_RM=slurm'
  20_mark:
    comment: 'Do not mark nodes offline'
    nodes:   '*'
    command: 'export MARK_OFFLINE=1'
  20_check:
    comment: 'In out-of-band contexts, enable all checks'
    nodes:   '*'
    command: 'export NHC_CHECK_ALL=1'
  eth0:
    comment: 'Checks for an active ethernet interface named "eth0."'
    nodes:   '*'
    command: 'check_hw_eth eth0'
  mcelog:
    comment: 'Check the mcelog daemon for any pending errors'
    nodes:   '*'
    command: 'check_hw_mcelog'
  rootrw:
    comment: 'All nodes should have their root filesystem mounted read/write'
    nodes:   '*'
    command: 'check_fs_mount_rw -f /'
  rootfree:
    comment: "Make sure the root filesystem doesn't get too full"
    nodes:   '*'
    command: 'check_fs_free / 3%'
  tmpfree:
    comment: "Make sure the /tmp filesystem doesn't get too full"
    nodes:   '*'
    command: 'check_fs_free /tmp 3%'
  home-stat:
    comment: 'Assert that /home filesystem is available'
    nodes:   '*'
    command: "check_file_test -d %{hiera('home_dir')}"
  homefree:
    comment: "Make sure the /home filesystem doesn't get too full"
    nodes:   '*'
    command: "check_fs_free %{hiera('home_dir')} 3%"
  tmprw:
    comment: 'These should always be directories and always be read/write/execute and sticky'
    nodes:   '*'
    command: 'check_file_test -r -w -x -d -k /tmp /var/tmp'
  devnull:
    comment: 'Assert common properties for /dev/null (which occasionally gets clobbered)'
    nodes:   '*'
    command: 'check_file_test -c -r -w /dev/null /dev/zero'

##### CLARA

# repository mirroring settings

clara_debian_mirror_server:  'ftp.debian.org'
clara_scibian_mirror_server: 'ftp.scibian.org'
clara_base_repos_root:       "%{hiera('website_dir')}/mirror"
clara_images_config_dir:     '/etc/clara/live'

profiles::clara::repos:
  # scibian8
  'jessie':
    suite_name:  'jessie'
    mirror_dir:  'debian'
    server:      "%{hiera('clara_debian_mirror_server')}"
    mirror_root: "%{hiera('clara_base_repos_root')}/scibian8/"
    sections:    'main,contrib,non-free,main/debian-installer'
  'jessie-updates':
    suite_name:  'jessie-updates'
    mirror_dir:  'debian'
    server:      "%{hiera('clara_debian_mirror_server')}"
    mirror_root: "%{hiera('clara_base_repos_root')}/scibian8/"
  'jessie-security':
    suite_name:  'jessie/updates'
    mirror_dir:  'debian-security'
    server:      "%{hiera('clara_debian_mirror_server')}"
    mirror_root: "%{hiera('clara_base_repos_root')}/scibian8/"
  'jessie-backports':
    suite_name:  'jessie-backports'
    mirror_dir:  'debian'
    server:      "%{hiera('clara_debian_mirror_server')}"
    mirror_root: "%{hiera('clara_base_repos_root')}/scibian8/"
  'scibian8':
    suite_name:  'scibian8'
    mirror_dir:  'scibian8/scibian'
    server:      "%{hiera('clara_scibian_mirror_server')}"
    mirror_root: "%{hiera('clara_base_repos_root')}/scibian8/"

clara::common_options:
  origin:      "%{hiera('clara_repo_originator')}"
clara::repo_options:
  gpg_key:     "%{hiera('clara_repo_gpg_key')}"
  clustername: "%{hiera('cluster_name')}"
  mirror_root: "%{hiera('profiles::http::mirror::docroot')}"
clara::ipmi_options:
  conmand:     "%{hiera('cluster_prefix')}%{my_conman_server}"
clara::keyring_source: "%{hiera('private_files_dir')}/repo/cluster_keyring.secret.gpg.enc"
clara::password_options:
  ASUPASSWD:            "%{hiera('cluster_decrypt_password')}"

# internal repository settings

clara_repo_originator:  'TeamHPC'
clara_repo_gpg_key:     '01234567'

##### Ftp
proftpd::user_home: "/srv/ftp"

##### APT config
apt::always_apt_update:    true
apt::disable_keys:         false
apt::purge_sources_list:   true
apt::purge_sources_list_d: true
apt::purge_preferences_d:  true
apt::update_timeout:       10
apt::proxy_host:           "apt.service.%{hiera('virtual_domain')}"
apt::proxy_port:           3142

##### Slurmweb
slurmweb::decrypt_passwd: "%{hiera('cluster_decrypt_password')}"
profiles::http::slurmweb::config_options:
  infos:
    version:
      comment: ''
      value:   '2.0'
  cors:
    authorized_origins:
      comment: ''
      value:   'https://your_dashboard'
  config:
    server_name:
      comment: ''
      value:   "%(hostname)"
    secret_key:
      comment: ''
      value:   'none'
    authentication:
      comment: ''
      value:   'disable'
    cache:
      comment: ''
      value:   'disable'
    racksxml:
      comment: ''
      value:   '/etc/slurm-web/racks.xml'
  roles:
    all:
      comment: ''
      value:   'all'
    user:
      comment: ''
      value:   "@%{hiera('user_group')}"
    admin:
      comment: ''
      value:   "@%{hiera('admin_group')}"
    restricted_fields_for_all:
      comment: ''
      value:   'command'
    restricted_fields_for_user:
      comment: ''
      value:   'command'
    restricted_fields_for_admin:
      comment: ''
      value:   ''
  acl:
    3dview:
      comment: ''
      value:   "@%{hiera('admin_group')}"

##### Monitoring

monitoring_master: "monitoring-master.${hiera('domain')}"
monitoring_server_primary: "%{hiera('cluster_prefix')}%{::my_monitoring_server}1.%{hiera('domain')}"
monitoring_server_secondary: "%{hiera('cluster_prefix')}%{::my_monitoring_server}2.%{hiera('domain')}"

icinga2::crt_host_src: "%{hiera('private_files_dir')}/icinga2/certs/%{::hostname}.crt"
icinga2::key_host_src: "%{hiera('private_files_dir')}/icinga2/certs/%{::hostname}.key.enc"
icinga2::crt_ca_src: "%{hiera('private_files_dir')}/icinga2/certs/ca.crt"
icinga2::decrypt_passwd: "%{hiera('cluster_decrypt_password')}"

profiles::monitoring::server::packages:
  - 'icinga2'
  - 'monitoring-plugins-basic'
  - 'monitoring-plugins-hpc-agent'
  - 'monitoring-plugins-hpc-server'

profiles::monitoring::server::features:
  - api
  - livestatus
  - command

profiles::monitoring::agent::features:
  - api

profiles::monitoring::features_conf:
  api:
    accept_config: true
    accept_commands: true

profiles::monitoring::server::endpoints:
  "%{hiera('monitoring_master')}": {}
  "%{hiera('monitoring_server_primary')}":
    host: "%{hiera('cluster_prefix')}%{::my_monitoring_server}1.%{hiera('domain')}"
  "%{hiera('monitoring_server_secondary')}":
    host: "%{hiera('cluster_prefix')}%{::my_monitoring_server}2.%{hiera('domain')}"

profiles::monitoring::server::zones:
  master:
    endpoints:
      - "%{hiera('monitoring_master')}"
  "%{hiera('cluster_name')}":
    parent: 'master'
    endpoints:
      - "%{hiera('monitoring_server_primary')}"
      - "%{hiera('monitoring_server_secondary')}"

profiles::monitoring::agent::ext:endpoints:
  "%{::fqdn}":
    host: "%{::fqdn}"
  "%{hiera('monitoring_master')}": {}

profiles::monitoring::agent::ext::zones:
  master:
    endpoints:
      - "%{hiera('monitoring_master')}"
  "%{::fqdn}":
    parent: 'master'
    endpoints:
      - "%{::fqdn}"

profiles::monitoring::agent::int::endpoints:
  "%{::fqdn}":
    host: "%{::fqdn}"
  "%{hiera('monitoring_server_primary')}": {}
  "%{hiera('monitoring_server_secondary')}": {}

profiles::monitoring::agent::int::zones:
  "%{hiera('cluster_name')}":
    endpoints:
      - "%{hiera('monitoring_server_primary')}"
      - "%{hiera('monitoring_server_secondary')}"
  "%{::fqdn}":
    parent: "%{hiera('cluster_name')}"
    endpoints:
      - "%{::fqdn}"

nscang_password: "CHANGEME"
nscang_identity: "%{hiera('cluster_name')}-agent-checker"

nscang::server::identity: "%{hiera('nscang_identity')}"
nscang::server::password: "%{hiera('nscang_password')}"

nscang::client::server: "%{hiera('cluster_prefix')}%{::my_monitoring_server}.%{hiera('domain')}"
nscang::client::identity: "%{hiera('nscang_identity')}"
nscang::client::password: "%{hiera('nscang_password')}"

##### Logs
profiles::log::client::remote_type: 'relp'
profiles::log::client::log_local:   true
profiles::log::client::server:      "%{hiera('cluster_prefix')}%{::my_log_server}"
profiles::log::client::port:        '20514'
rsyslog::client::spool_size:        '200m'
# This log_server_dir shared parameter is used only once in this hiera layer but
# has proven to be useful in uper layers, typically to define logrotate rules.
log_server_dir:                     "%{hiera('admin_dir')}/restricted/logs"
rsyslog::server::server_dir:        "%{hiera('log_server_dir')}/"
rsyslog::server::custom_config:     'rsyslog/server-hostname.conf.erb'

##### Consul
consul::domain: "%{hiera('virtual_domain')}"
consul::datacenter: "%{hiera('cluster_name')}"

profiles::consul::client::packages:
  - consul
  - consult
profiles::consul::server::packages:
  - consul
profiles::consul::server::subservices:
  - name: 'http'
    check:
      id: 'http_check'
      name: 'Local HTTP service check'
      http: 'http://localhost/'
      interval: '10s'
      timeout: '1s'
  - name: 'apt'
    check:
      id: 'apt_check'
      name: 'Local HTTP service check'
      http: "http://localhost:%{hiera('apt::proxy_port')}/acng-report.html"
      interval: '10s'
      timeout: '1s'
  - name: 'tftp'
    check:
      id: 'tftpd_check'
      name: 'Local TFTPD service check'
      script: '/usr/lib/nagios/plugins/check_procs -c 1: --command=in.tftpd'
      interval: '30s'
      timeout: '10s'
  - name: 's3-system'
    check:
      id: 'rgw_s3_system_check'
      name: 'Local Rados GW System S3 bucket check'
      http: 'http://localhost:7480/'
      interval: '10s'
      timeout: '1s'
  - name: 'rgw'
    check:
      id: 'rgw_check'
      name: 'Local Rados GW service check'
      http: 'http://localhost:7480/'
      interval: '10s'
      timeout: '1s'
  - name: 'secret'
    check:
      id: 'http_secret_check'
      name: 'Local HTTP secret check'
      http: "http://localhost:%{hiera('secret_port')}/"
      interval: '10s'
      timeout: '1s'
  - name: 'web-boot'
    check:
      id: 'http_boot_check'
      name: 'Local HTTP bootsystem check'
      http: "http://localhost:%{hiera('boot_http_port')}/"
      interval: '10s'
      timeout: '1s'
# Slurmweb
slurmweb::racks_file:  '/etc/slurm-web/racks.xml'
slurmweb::secret_file: '/etc/slurm-web/secret.key'


##### Backup #####
profiles::backup::collect_dir::sources:
  'slurmdbd':
    source_profiles:
      - 'jobsched::server'
    source_dir: '/var/backups/slurmdbd'

hpc_backup::collect_dir::base_dir: "/var/backups/%{hiera('cluster_name')}"
