primary_server: 'batch1'
secondary_server: 'batch2'
slurm_user: 'slurm'
slurmd_user: 'root'
slurmcommons::slurm_conf_options:
    ClusterName: 
      value:   "%{hiera('cluster_name')}"
      comment: "The name by which this SLURM managed cluster is known in the accounting database"
    ControlMachine:
      value:   "%{hiera('primary_server')}"
      comment: "Hostname of the machine where SLURM control functions are executed"
    BackupController:
      value:   "%{hiera('secondary_server')}"
      comment: "Machine where SLURM control functions are to be executed if ControlMachine fails"
    SlurmUser:
      value:   "%{hiera('slurm_user')}"
      comment: "The name of the user that the slurmctld daemon executes as"
    SlurmdUser:
      value:   "%{hiera('slurmd_user')}"
      comment: "The name of the user that the slurmd daemon executes as"
    SlurmctldPort:
      value:   "6817"
      comment: "The port number that the SLURM controller slurmctld listens to for work"
    SlurmdPort:
      value:   "6818"
      comment: "The port number that the SLURM compute node daemon slurmd listens to for work. SlurmctldPort and SlurmdPort must be different"
    SlurmctldPidFile:
      value:   "/var/run/slurm-llnl/slurmctld.pid"
      comment: "File into which the slurmctld daemon may write its process id"
    SlurmdPidFile:
      value:   "/var/run/slurm-llnl/slurmd.pid"
      comment: "File into which the slurmd daemon may write its process id"
    SlurmdSpoolDir:
      value:   "/var/spool/slurmd"
      comment: "Directory (local file system) into which the slurmd daemons state information and batch job script information are written"
    AuthType:
      value:   "auth/munge"
      comment: "The authentication method for communications between SLURM components"
    CryptoType:
      value:   "crypto/munge"
      comment: "The cryptographic signature tool to be used in the creation of job step credentials"
    DisableRootJobs:
      value:   "YES"
      comment: "If set to 'YES' then user root will be prevented from running any jobs"
    CacheGroups:
      value:   "1"
      comment: "If set to 1 the slurmd daemon will cache /etc/groups entries. This can improve performance for highly parallel jobs"
    GroupUpdateForce:
      value:   "1"
      comment: "If set to 1 then information about which users are members of groups allowed to use a partition will be updated periodically"
    GroupUpdateTime:
      value:   "600"
      comment: "Controls how frequently information about which users are members of groups allowed to use a partition will be updated"
    UsePAM:
      value:   "1"
      comment: "If set to 1 authentication by PAM will be enabled"
    SwitchType:
      value:   "switch/none"
      comment: "Identifies the type of switch or interconnect used for application communications. Acceptable values include 'switch/none' and 'switch/elan'"
    TopologyPlugin:
      value:   "topology/tree"
      comment: "Identifies the plugin to be used for determining the network topology and optimizing job allocations to minimize network contention. Used for a hierarchical network as described in a topology.conf file"
    TreeWidth:
      value:   "600"
      comment: "Slurmd daemons use a virtual tree network for communications. IMPORTANT: configure TreeWidth to a number at least as large as the maximum node count!!"
    Prolog:
      value:   "/usr/lib/slurm/generic-scripts/Prolog.sh"
      comment: "Script executed at job step initiation on that node"
    PrologSlurmctld:
      value:   "/usr/lib/slurm/generic-scripts/PrologSlurmctld.sh"
      comment: "Script executed at job allocation"
    TaskProlog:
      value:   "/usr/lib/slurm/generic-scripts/TaskProlog.sh"
      comment: "Script executed at job step initiation by user invoking srun command"
    SrunProlog:
      value:   "/usr/lib/slurm/generic-scripts/SrunProlog.sh"
      comment: "Script executed at job step initiation by user invoking sbatch command"
    Epilog:
      value:   "/usr/lib/slurm/generic-scripts/Epilog.sh"
      comment: "Script executed at job termination by %{hiera('slurmd_user')}"
    EpilogSlurmctld:
      value:   "/usr/lib/slurm/generic-scripts/EpilogSlurmctld.sh"
      comment: "Script executed at job termination by %{hiera('slurm_user')}"
    TaskEpilog:
      value:   "/usr/lib/slurm/generic-scripts/TaskEpilog.sh"
      comment: "Script executed at completion job step by user invoking sbatch command"
    SrunEpilog:
      value:   "/usr/lib/slurm/generic-scripts/SrunEpilog.sh"
      comment: "Script executed at completion job step by user invoking srun command"
    HealthCheckProgram:
      value:   "/usr/sbin/nhc"
      comment: "Script to execute as user root periodically on all compute nodes. This may be used to verify the node is fully operational and DRAIN the node if a problem is detected."
    HealthCheckInterval:
     value:   "300"
     comment: "The interval in seconds between executions of HealthCheckProgram. If value is zero execution will be disabled"
    MailProg:
      value:   "/usr/bin/mail"
      comment: "Program used to send email per user request"
    BatchStartTimeout:
      value:   "10"
      comment: "The maximum time (in seconds) that a batch job is permitted for launching before being considered missing and releasing the allocation"
    CompleteWait:
      value:   "0"
      comment: "The time in seconds given for a job to remain in COMPLETING state before any additional jobs are scheduled"
    KillWait:
     value:   "30"
     comment: "The interval in seconds given to a job's processes between the SIGTERM and SIGKILL signals upon reaching its time limit"
    EpilogMsgTime:
     value:   "2000"
     comment: "The number of microseconds that the slurmctld daemon requires to process an epilog completion message from the slurmd dameons"
    HealthCheckInterval:
      value:   "300"
      comment: "The interval in seconds between executions of HealthCheckProgram. If value is zero execution will be disabled"
    InactiveLimit:
      value:   "0"
      comment: "IMPORTANT : Script /usr/local/sbin/slurm_srun_prolog.visu require set up this value to zero"
    MessageTimeout:
      value:   "30"
      comment: "Time permitted for a round-trip communication to complete in seconds"
    ResvOverRun:
      value:   "10"
      comment: "Describes how long (in minutes] a job already running in a reservation should be permitted to execute after the end time of the reservation has been reached"
    MinJobAge:
      value:   "300"
      comment: "The minimum age of a completed job before its record is purged from SLURM's active database"
    MaxJobCount:
      value:   "5000"
      comment: "The maximum number of jobs SLURM can have in its active database at one time"
    FirstJobId:
      value:   "1"
      comment: "The job id to be used for the first submitted to SLURM without a specific requested value"
    MaxJobId:
      value:   "4294901760"
      comment: "The maximum job id to be used for jobs submitted to SLURM without a specific requested value"
    OverTimeLimit:
      value:   "3"
      comment: "Number of minutes by which a job can exceed its time limit before being canceled"
    SlurmctldTimeout:
      value:   "60"
      comment: "The interval in seconds that the backup controller waits for the primary controller to respond before assuming control"
    SlurmdTimeout:
      value:   "120"
      comment: "The interval in seconds that the SLURM controller waits for slurmd to respond before configuring that node's state to DOWN"
    UnkillableStepTimeout:
      value:   "60"
      comment: "The length of time in seconds that SLURM will wait before deciding that processes in a job step are unkillable (after they have been signaled with SIGKILL)"
    vSizeFactor:
      value:   "0"
      comment: "The job will be killed if its real memory exceeds 500MB or its virtual memory exceeds 505MB. If set to 0 this feature is disabled."
    waittime:
      value:   "0"
      comment: "Specifies how many seconds the srun command should by default wait after the first task terminates before terminating all remaining tasks. If set to 0 this feature is disabled."
    DefMemPerCPU:
      value:   "0"
      comment: "Default real memory size available per allocated CPU in MegaBytes. 0 == unlimited."
    MaxMemPerCPU:
      value:   "0"
      comment: "Maximum real memory size available per allocated CPU in MegaBytes. 0 == unlimited."
    FastSchedule:
      value:   "1"
      comment: "TO REVIEW"
    SchedulerRootFilter:
      value:   "1"
      comment: "If set to 0 then RootOnly partitions are treated like any other partition. If set to 1 then RootOnly partitions are exempt from any external scheduling activities"
    SchedulerTimeSlice:
      value:   "30"
      comment: "Number of seconds in each time slice when gang scheduling is enabled (PreemptMode=GANG)"
    SchedulerType:
      value:   "sched/backfill"
      comment: "sched/wiki or sched/wiki2: for the Wiki interface to the Maui Schedule"
    SelectType:
      value:   "select/cons_res"
      comment: "select/cons_res: The resources within a node are individually allocated as consumable resources. Note that whole nodes can be allocated to jobs for selected partitions by using the Shared=Exclusive option. Defines specific subsystems which should provide more detailed event logging. Valid subsystems available today (with more to come) include:"
    SelectTypeParameters:
      value:   "CR_CPU_Memory"
      comment: ""
    PropagatePrioProcess:
      value:   "0"
      comment: "2: Like 1 with the restriction that their nice value will always be one higher than the slurm daemon"
    PropagateResourceLimits:
      value:   "NONE"
      comment: "The following limit names are supported by SLURM: ALL NONE AS CORE CPU DATA FSIZE MEMLOCK NOFILE NPROC RSS STACK. A list of comma separated resource limit names to exclude"
    CheckpointType:
      value:   "checkpoint/none"
      comment: "Possibles values for GNU/Linux are checkpoint/blcr; checkpoint/ompi; checkpoint/none "
    StateSaveLocation:
      value:   "${slurm_dir}/StateSaveLocation"
      comment: "Directory into which the SLURM controller (slurmctld) saves its state"
    JobRequeue:
      value:   "1"
      comment: ""
    EnforcePartLimits:
      value:   "YES"
      comment: "If set to 'YES' then jobs which exceed a partition's size and/or time limits will be rejected at submission time"
    JobSubmitPlugins:
      value:   "lua"
      comment: "A comma delimited list of job submission plugins to be used. The specified plugins will be executed in the order listed."
    MpiDefault:
      value:   "none"
      comment: "Identifies the default type of MPI to be used. Currently supported versions include: lam mpich1_p4 mpich1_shmem mpichgm mpichmx mvapich none and openmpi"
    TmpFs:
      value:   "/tmp"
      comment: "Pathname of the file system available to user jobs for temporary storage"
    JobFileAppend:
      value:   "0"
      comment: ""
    KillOnBadExit:
      value:   "0"
      comment: "Users can override this configuration parameter by using srun -K --kill-on-bad-exit."
    MaxStepCount:
      value:   "100000"
      comment: "The maximum number of steps that any job can initiate"
    MaxTasksPerNode:
      value:   "28"
      comment: "Maximum number of tasks SLURM will allow a job step to spawn on a single node"
    TaskPlugin:
      value:   "task/cgroup"
      comment: "'task/affinity' 'task/cgroup' or 'task/none'"
    ProctrackType:
      value:   "proctrack/cgroup"
      comment: "Identifies the plugin to be used for process tracking"
    PriorityType:
      value:   "priority/multifactor"
      comment: "This specifies the plugin to be used in establishing a jobs scheduling priority. Supported values are 'priority/basic' and 'priority/multifactor'"
    PriorityCalcPeriod:
      value:   "5"
      comment: "The period of time in minutes in which the half-life decay will be re-calculated. Applicable only if PriorityType=priority/multifactor"
    PriorityUsageResetPeriod:
      value:   "NONE"
      comment: "At this interval the usage of associations will be reset to 0. Supported values are: NONE NOW DAILY WEEKLY MONTHLY QUARTERLY YEARLY"
    PriorityDecayHalfLife:
      value:   "14-0"
      comment: "This controls how long prior resource use is considered in determining how over- or under-serviced an association is in determining job priority. The unit is a time string (days-hr:min:00 or days-hr)"
    PriorityFavorSmall:
      value:   "NO"
      comment: "Specifies that small jobs should be given preferential scheduling priority"
    PriorityMaxAge:
      value:   "14-0"
      comment: "Specifies the job age which will be given the maximum age factor in computing priority"
    PriorityWeightAge:
      value:   "1000"
      comment: "An integer value that sets the degree to which the queue wait time component contributes to the job's priority"
    PriorityWeightFairshare:
      value:   "1000"
      comment: "An integer value that sets the degree to which the fair-share component contributes to the job's priority"
    PriorityWeightJobSize:
      value:   "1000"
      comment: "An integer value that sets the degree to which the job size component contributes to the job's priority."
    PriorityWeightQOS:
      value:   "1000"
      comment: "An integer value that sets the degree to which the Quality Of Service component contributes to the job's priority"
    PriorityWeightPartition:
      value:   "1000"
      comment: "An integer value that sets the degree to which the node partition component contributes to the job's priority"
    AccountingStorageType:
      value:   "accounting_storage/slurmdbd"
      comment: "'accounting_storage/filetxt' 'accounting_storage/mysql' 'accounting_storage/none' 'accounting_storage/pgsql' and 'accounting_storage/slurmdbd'"
    AccountingStorageHost:
      value:   "%{hiera('primary_server')}"
      comment: "The name of the machine hosting the accounting storage database"
    AccountingStorageBackupHost:
      value:   "%{hiera('secondary_server')}"
      comment: "The name of the backup machine hosting the accounting storage database"
    AccountingStorageEnforce:
      value:   "qoslimits"
      comment: "This controls what level of association-based enforcement to impose on job submissions. Valid options are any combination of associations limits qos and wckeys or all for all thing"
    TrackWCKey:
      value:   "yes"
      comment: "If you want to track wckeys (Workload Characterization Key) of users."
    JobCompType:
      value:   "jobcomp/none"
      comment: "The job completion logging mechanism type. Acceptable values at present include 'jobcomp/none' 'jobcomp/filetxt' 'jobcomp/mysql' 'jobcomp/pgsql' and 'jobcomp/script'"
    JobAcctGatherType:
      value:   "jobacct_gather/linux"
      comment: "The job accounting mechanism type. Acceptable values at present include 'jobacct_gather/aix' 'jobacct_gather/linux' and 'jobacct_gather/none'"
    JobAcctGatherFrequency:
      value:   "60"
      comment: "A value of zero disables real the periodic job sampling and provides accounting information only on job termination. "
    SlurmctldDebug:
      value:   "3"
      comment: "The level of detail to provide slurmctld daemon's logs. Values from 0 to 9 are legal"
    SlurmdDebug:
      value:   "3"
      comment: "The level of detail to provide slurmd daemon's logs. Values from 0 to 9 are legal"
    SlurmctldLogFile:
      value:   "/var/log/slurm-llnl/slurmctld.log"
      comment: "Log file for slurmctld daemon"
    SlurmdLogFile:
      value:   "/var/log/slurm-llnl/slurmd.log"
      comment: "Log file for slurmd daemon"
    ReturnToService:
      value:   "1"
      comment: "Controls when a DOWN node will be returned to service. 1 = A DOWN node will become available for use upon registration with a valid configuration only if it was set DOWN due to being non-responsive"
    SuspendTime:
      value:   "-1"
      comment: "Nodes becomes eligible for power saving mode after being idle for this number of seconds"
    Include:
      value:   "/etc/slurm-llnl/partitions.conf"
      comment: "If a line begins with the word 'Include' followed by whitespace and then a file name, that file will be included inline with the current configuration file"
