= Software architecture

include::arch/pattern.asc[]

include::arch/hiera.asc[]

[[arch-internalrepo]]
== Internal repository

////
TODO: explain Puppet-HPC must be used in combination with a another internal
repository. Highly recommended content and structure:

* internal modules
* internal files
* internal hieradata
////

Puppet-HPC cannot be used only on its own, it must be configured for a specific
site and a specific cluster. It is recommended to work with an additional
internal repository that will contain all specific data that cannot be
published on GitHub.

[[img-internal_repo_combination]]
.Combination of Puppet-HPC with internal repository
image::src/img/internal_repository_combination.pdf[width=500]

The content and structure of this internal repository is explained below:

* `files` contains any configuration file that needs to be stored internally
and that can be used as it is in a cluster configuration. It can be, for example
, SSL certificates, SSH host keys, etc. If the content of these files is
sensitive, they should be encrypted.
* `hieradata` contains all the site-specific and cluster-specific data
necessary to configure a cluster with Puppet-HPC.
* `puppet-config` includes some Puppet configuration files to use with a
specific cluster such as puppet.conf, hiera.yaml.

For each of these 3 directories, it is recommended to have a subdirectory per
cluster being configured with Puppet-HPC.

[[arch-deps]]
== External dependencies

Puppet-HPC provides a set of Puppet modules but also relies on some Puppet
external community modules that can be considered as dependencies. The full
list of these modules is:

* https://forge.puppet.com/puppetlabs/stdlib[`puppetlabs-stdlib`]
* https://forge.puppet.com/puppetlabs/concat[`puppetlabs-concat`]
* https://forge.puppet.com/puppetlabs/apache[`puppetlabs-apache`]
* https://forge.puppet.com/puppetlabs/apt[`puppetlabs-apt`]
* https://forge.puppet.com/arioch/keepalived[`arioch-keepalived`]
* https://forge.puppet.com/herculesteam/augeasproviders[`herculesteam-augeasproviders-core`]
* https://forge.puppet.com/puppet/archive[`puppet-archive`]
* https://forge.puppet.com/puppet/collectd[`puppet-collectd`]
* https://forge.puppet.com/saz/rsyslog[`saz-rsyslog`]
* https://forge.puppet.com/yo61/logrotate[`yo61-logrotate`]

It is also strongly recommended to install
https://github.com/TomPoulton/hiera-eyaml[eyaml] utility in order to encrypt
sensitive data (such as passwords) in Hiera YAML files.

[[arch-genericity]]
== Genericity levels

The use of the Roles/Profiles pattern enables to control the level of
genericity of each element of a Puppet configuration code base. Here are the
genericity levels defined for all components in the Puppet-HPC project:

* Roles: The code part of the roles is fully generic as it consists into one
  manifest (located under path `puppet-config/cluster/roles/manifests/init.pp`)
  which simply extract from Hiera the list of profiles included in the role.
  However, the name of the roles and the list of profiles are cluster specific
  due to technical properties of Puppet-HPC. This point is explained further in
  the <<arch-roles,roles section>>.
* Profiles are HPC specific and are highly related to the way Scibian HPC
  clusters are modelized. Nevertheless they can be reused from one cluster to
  the other. Their structure should follow the reference architecture defined in
  https://github.com/edf-hpc/scibian-hpc-install-guide[Scibian HPC cluster
  installation guide].
* Modules are fully generic. They support multiple distributions and can even be
  used outside of the HPC clusters context when relevant.

As stated in <<arch-hieralayers,Hiera layers section>>, the Hiera repository is
composed of multiple layers of datasets ordereded by genericity levels. Then,
each layer of the hierarchy has it own genericity level depending on its
specialization.

This diagram gives quick glance summary of the genericity levels for each
element of the Puppet-HPC stack:

[[img-components_genericity]]
.Stack components genericity goals
image::src/img/components_genericity.pdf[width=500]

[[arch-roles]]
== Roles

As previously stated in <<arch-pattern,pattern section>>, a node has exactly one
role representing its _business logic_, a role being nothing more than a set of
profiles. By design, all machines sharing the same role have the same set of
profiles. In other words, if it is necessary to apply a different set of
profiles to different machines, they must have different roles

In Puppet-HPC, the node to role association is set using a custom fact
`puppet_role` provided by the `hpclib` module. The fact actually extracts the
role name out of the hostname of the node, using the following pattern:

----
<prefix><role><id>
----

Where:

* `<prefix>` is the prefix of the cluster name (as provided by Hiera),
* `<role>` is the role name,
* `<id>` is a set of consecutive digits.

For example, on a cluster whose _prefix_ is `foo`, the role names extracted from
the following hostnames are:

* `foobar1`: `bar`
* `foocompute001`: `compute`
* `fooservice2boot001`: `service2boot`

This fact is then used in <<arch-hieralayers,Hiera layers definitions>> to get a
role specific layer. The parameters values defined in this layer are specific to
the nodes having this role. This role specific layer of the Hiera repository is
primarly designed to set the list of profiles associated to the role, under the
generic `profiles` parameter name. As example, here is a possible value of this
parameter in file `<privatedata>/hieradata/<cluster_name>/roles/cn.yaml`:

----
profiles:
  - profiles::cluster::common
  - profiles::network::base
  - profiles::dns::client
  - profiles::access::base
  - profiles::ntp::client
  - profiles::ssmtp::client
  - profiles::jobsched::exec
  - profiles::openssh::server
  - profiles::environment::base
  - profiles::auth::client
  - profiles::metrics::collect_base
  - profiles::log::client
----

This parameter is extracted from Hiera repository by the fully generic main
manifest of roles module `puppet-config/cluster/roles/manifests/init.pp`:

[source]
----
include::../../puppet-config/cluster/roles/manifests/init.pp[]
----

In order to work as expected, this mechanism has the following requirements:

* All nodes must follow this naming conventions.
* The prefix of the cluster name must be set with `cluster_prefix` parameter at
  the cluster level in Hiera repository.
* The `profiles` parameter must be defined at the role specific layer of the
  Hiera repository, for all possible roles.

This diagrams gives a summary of this node/role/profiles associations logic:

[[img-node-role-profiles-assoc]]
.Node, role and profiles associations mechanism
image::src/img/node_role_profiles_assoc.pdf[width=500]

== Cluster definition

////
TODO: how-to write a cluster definition in hiera.

* Main shared parameters
* Network parameters
* Cluster name. Also explain why not in hiera.
////

This cluster configuration is meant to be used with a standard cluster
architecture, deviation from this architecture should be minimum. Some
constraints are planned to be relaxed in the future.

Here, we are going to describe this architecture and how it should be
defined to be used by the Puppet-HPC configuration.

[[cluster-architecture]]
=== Architecture

image::src/img/cluster_architecture.png[image]

As noted earlier, each machine is defined by its _role_ (front, cn, 
batch, etc ...). We can separate the machines into two groups : nodes with user
access (frontends, compute nodes, etc ..) and nodes who provides services for 
the cluster, including storage. There's no particular distinction (except 
the _role_) between these two groups in the configuration.

[[cluster-main-parameters]]
=== Main shared parameters

Some parameters must be defined at the _cluster_ level of the Hiera hierarchy.
These variables are not associated to a unique profile and are aimed to be
reused directly several times in Hiera or in puppet profiles.

* `cluster_name`: The complete name of the cluster. Can be used in slurm 
configuration, for example.
* `cluster_prefix`: The prefix used for all the hostnames in the cluster. 
Generally it will be composed of 2 or 3 letters ("gen" for a cluster 
named "generic", for example).
* `private_files_dir`: The directory where all the files copied by puppet on the
machines are stored. These files can be encrypted or not.
It can be a shared directory between all the nodes, or an http export if the 
_hpclib::hpc_file_ resource is used.
* `domain`: The domain name used across all the machines. Used in particular by the
_bind_ module.  
* `user_groups`: Array of user groups authorized to connect and submit jobs to
the cluster.
* `cluster_decrypt_password`: General password used by the _hpclib_ module to 
decrypt encrypted files before copying them on the machines. This variable is
usually itself encrypted using e-yaml.

[[cluster-network-definitions]]
=== Network definitions

[[cluster-topology]]
==== Topology

The network topology is defined at the _cluster_ level of the Hiera
hierarchy. This means it is common to all nodes.

----
## Network topology of the cluster
net::administration::ipnetwork: '172.16.0.0'
net::administration::netmask: '255.255.248.0'
net::administration::prefix_length: '/21'
net::administration::broadcast: '172.16.7.255'
net::lowlatency::ipnetwork: '172.16.40.0'
net::lowlatency::prefix_length: '/21'
net::management::ipnetwork: '172.16.80.0'
net::management::netmask: '255.255.240.0'
net::management::broadcast: '172.16.95.255'
net_topology:
    'wan':
        'name':           'WAN'
        'prefixes':       'wan'
        'ipnetwork':      '172.17.0.0.0'
        'netmask':        '255.255.255.0'
        'prefix_length':  '/24'
        'gateway':        '172.17.0.1'
        'broadcast':      '172.17.0.255'
        'ip_range_start': '172.17.0.1'
        'ip_range_end':   '172.17.0.254'
        'firewall_zone':  'wan'
    'administration':
        'name':           'ADM'
        'prefixes':       ''
        'ipnetwork':      '172.16.0.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':        '172.16.0.1'
        'broadcast':      '172.16.7.255'
        'ip_range_start': '172.16.0.1'
        'ip_range_end':   '172.16.7.254'
        'firewall_zone':  'clstr'
        'pool0':
            'ip_range_start':
                            '172.16.0.1'
            'ip_range_end':
                            '172.16.5.254'
        'pool1':          # IP reserved for the discovery process
            'ip_range_start':
                            '172.16.6.1'
            'ip_range_end':
                            '172.16.7.254'
    'lowlatency':
        'name':           'IB'
        'prefixes':       'ib'
        'ipnetwork':      '172.16.40.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':         ''
        'broadcast':       '172.16.47.255'
        'ip_range_start':  '172.16.40.1'
        'ip_range_end':    '172.16.47.254'
        'firewall_zone':   'clstr'
    'management':
        'name':            'MGT'
        'prefixes':        'mgt'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.240.0'
        'prefix_length':   '/20'
        'gateway':         ''
        'broadcast':       '172.16.95.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.95.254'
        'firewall_zone':   'clstr'
    'bmc':
        'name':            'BMC'
        'prefixes':        'bmc'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.248.0'
        'prefix_length':   '/21'
        'gateway':         ''
        'broadcast':       '172.16.87.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.87.254'
        'firewall_zone':   'clstr'
----

The `bmc` network connect all the management cards (bmc, imm, etc ...). The
`management` network connect the servers who must access these management
devices. That is the reason why they share an IP networks settings and ranges.

[[cluster-bonding]]
==== Bonding

Some network interfaces are bonded together for load balancing and high
availability. The bonding definition is done in Hiera. If the bonding is
uniform (i.e. same bond interface on same slave interfaces) between nodes,
this can be done at the _cluster_ level. In case of differences between
nodes, it must be done higher in the hierarchy (_role_ or _node_).

----
network::bonding_options:
    'bond0':
      'slaves':
         - 'eth0'
         - 'eth1'
      'options':  'mode=active-backup primary=eth0'
    'bond1':
      'slaves':
         - 'eth2'
         - 'eth3'
      'options':  'mode=active-backup primary=eth2'
----

This variable from Hiera uses _Auto Lookup_ to be passed to the `network`
class.

[[cluster-bridges]]
==== Bridges

When using a machine as a physical host for VMs, it is often necessary to
setup bridge interfaces. This bridge interfaces will be configured in the
`master_network` hash with the right IP addresses. The physical device will be
added automatically without an IP address. It is possible to create a bridge
above a bonding interface.

----
network::bridge_options:
    'br0':
      'ports':
         - 'bond0'
      'description': 'Administration network bridge.'
    'br2':
      'ports':
         - 'bond2'
      'description': 'WAN network bridge'
----

[[cluster-node-definitions]]
=== Node definitions

==== Master Network

Nodes are listed in a Hiera hash called `master_network`. It must be defined 
at the at the _cluster_ level of the Hiera hierarchy, but for readability
reasons it is in a separate `network.yaml` file.
Each key defines one node and its network configuration. Each node is described
by a hash containing its fully qualified hostname and the networks attached to 
it. Each network must have a name corresponding to the ones used in the  
`net_topology` hash described in the <<cluster-topology,topology section>>.

image::src/img/net_topology-master_network-corresponding.png[image]

These values can be defined for each network:

* MAC address (`DHCP_MAC`):
The MAC address of the physical device connected to the network. It is used
to build the dhcpd server configuration.

* Interface device (`device`):
The device where the configuration must be applied, this means that with a
bonded interface, the configuration must be applied on the `bond` interface.
So it is not necessary the physical interface is attached to the MAC address
quoted above. The interfaces enslaved to the `bond` interfaces can be
omitted from this configuration.

* Hostname (`hostname`):
The hostname of the machine on the considered network.

* IPv4 Address (`IP`):
The IPv4 address of the machine on the considered network. The netmask
comes from the `net_topology` variable.

* External config (or not) :
External configuration means the interface is configured on the system but
should not be setup by the Puppet-HPC configuration. It is useful if another
subsystem sets up the network interface: VPN, libvirt... On Debian, it means
the interface is not added to `/etc/network/interfaces`. This boolean can take
the value `true` or `false` and is considered `false` if omitted

Example:

----
master_network:
  genmisc1:
    fqdn: 'genmisc1.somedomain'
    networks:
      administration:
        'DHCP_MAC': '52:54:00:ba:9d:ac'
        'IP':       '172.16.2.21'
        'device':   'bond0'
        'hostname': 'genmisc1'
      lowlatency:
        'IP':       '172.16.42.21'
        'device':   'ib0'
        'hostname': 'llgenmisc1'
      management:
        'IP':       '172.16.88.21'
        'device':   'bond0'
        'hostname': 'mgtgenmisc1'
      bmc:
        'DHCP_MAC': '40:F2:E9:CD:53:CE'
        'IP':       '172.16.82.21'
        'hostname': 'bmcgenmisc1'
      wan:
        'IP':       '172.17.42.45'
        'device':   'bond1'
        'hostname': 'extgenmisc1'
----

This example defines one node (`genmisc1`) with the following
configuration:

* DHCP
** `52:54:00:ba:9d:ac` `genmisc1` `172.16.2.21`
** `40:F2:E9:CD:53:CE` `mgtgenmisc1` `172.16.82.21`
* Network configuration on the node
** `bond0` `172.16.2.21` `255.255.248.0` External Config: `false`
** `bond0` `172.16.88.21` `255.255.248.0` External Config: `false`
** `bond1` `172.17.42.45` `255.255.255.0` External Config: `false`
* DNS and Hosts
** `genmisc1` `172.16.2.21`
** `extgenmisc1` `172.17.42.45`

All lists are optional, so it is possible to define an element that just
defines a Host/DNS configuration (for virtual IP addresses for
instance):

----
master_network:
  genmisc:
    fqdn: 'genmisc.somedomain'
    networks:
      administration:
        'IP':       '172.16.2.20'
        'hostname': 'genmisc'
      management:
        'IP':       '172.16.82.20'
        'hostname': 'mgtgenmisc'
      wan:
        'IP':       '172.17.42.44'
        'hostname': 'extgenmisc'
----

== Deployment
////
TODO: document installation mode (packages, git clones) and push and apply
scripts with refs to manpages. Explain why not puppetserver mode.
////

Scibian clusters uses a simple _puppet apply_ command with a set of modules,
manifests and data. Puppet-hpc is not designed to work in a traditional
"Puppet server" environment, as it must be used in a very simple system
environment, like a post installation script inside Debian Installer.

=== Push and apply scripts

Two tools have been developed in order to apply the puppet-hpc configuration
on the nodes of a cluster. One "push" the entire configuration (modules, Hiera
data, files) in one shared space, and another one is aimed to apply this
configuration on the nodes.

image::src/img/config_push_apply.png[image]

==== hpc-config-push

This script merge all the data necessary to apply the puppet configuration in 
one archive and push it into a shared storage:

* Puppet modules installed in operating system via packages
* Puppet modules from the _puppet_hpc_ git repository, including profiles
* Optionally other puppet modules
* Configuration files for puppet and Hiera
* Yaml files for Hiera: generic ones from puppet-hpc git repository and
  specific ones from the internal repository
* Files to copy on nodes from the internal repository

Two methods can be used to push the data:
* *posix*: simply copies the tarball into a shared directory on all nodes
  (a nfs export, for example)
* *s3*: uses the Amazon S3 RESTful API to send data on a compatible storage
  (Ceph Rados Gateway, for example)

The script can manage several
https://docs.puppet.com/puppet/3.7/environments.html[Puppet environments]
with the `-e, --environment` parameter. A default environment can be defined in
the configuration file.

The file `/etc/hpc-config/push.conf` allows to configure all the options for
this script. See manpage for detailed description.

Please refer to `hpc-config-push(1)` manpage for full usage documentation.

==== hpc-config-apply

The apply script downloads the puppet configuration (modules and hieradata)
as a tarball and installs it as an environment in `/etc/puppet`. Private data
files are not downloaded with the configuration. Private data files are
available from the central storage and are directly downloaded from the puppet
modules.
If eyaml is used, this script needs a source to download the keys used to
encrypt and decrypt data.

The command `puppet apply` is executed afterward with the environment
previously untarred.

The configuration file indicating where to download the files is located in
_/etc/hpc-config.conf_. See the manpage for detailed description of the
options.

Please refer to `hpc-config-apply(1)` manpage for full usage documentation.

=== Packages

These two scripts are provided with Scibian distribution as debian packages for
Debian Jessie:

* `hpc-config-apply`
* `hpc-config-push`
