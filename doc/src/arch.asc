= Software architecture

== Pattern

Puppet-HPC is based on the three following tools and principles:

* *Facter* is used to report per-node facts. Morevover some facts specific to
the HPC context are used to convey the global information about the cluster
that needs to be known when running Puppet on a node. These facts are
implemented in the `hpclib` module.

* *Hiera* is used to look up data. This tool helps separating site-specific or
cluster-specific data from Puppet code. Specific data are excluded from
Puppet-HPC, being kept, versionned and maintained in a separate internal
repository. The cluster description it contains should follow certain rules
though. These rules are detailed below in the Cluster Definition part.

* *The Roles/Profiles pattern* has been used to design the Puppet-HPC code. It
is organized in different levels of abstraction:

** Roles, which represent the business logic. A node includes one role,
and one only. Each role lists one or more profiles.
** Profiles, which represent the implementation of technical functionalities.
A profile includes and manages modules to define a logical technical stack.
** Modules, which are the elementary technical blocks. Modules should only be
responsible for managing aspects of the component they are written for and
should be as generic as possible.

Regarding the Roles/Profiles pattern, it is a common pattern in Puppet code
organization. This pattern is explained in details in this presentation:
https://puppet.com/presentations/designing-puppet-rolesprofiles-pattern

[[img-roles_profiles_data_separation]]
.Code and data separation with roles and profiles pattern
image::src/img/pattern_roles_profiles_hiera.pdf[width=300]

One of the interesting aspects of the Roles/Profiles pattern is that modules
should be as generic as possible. Whenever it is possible, *external community
modules* should be used. It should come from a reliable source: distribution
package or the Puppetlabs forge. In any case, external community modules should
be properly reviewed.

[[arch-hieralayers]]
== Hiera layers

Hiera is a software designed to manage a repository of data formatted in
key/value pairs. The key is the parameter name. The values can be of various
types: string, numbers, booleans, hashes or arrays. Puppet-HPC requires to use
default Hiera YAML backend, therefore the data are stored in YAML files.

Hiera is able to look up data out of a hierarchy - hence its name - of layers
and manage overrides. This feature, combined with layers properly ordered by
genericity levels, allow to define a maximum number of parameters once for
multiple clusters and organization. The parameters are overriden in more
specialized layers only when necessary. The following diagram illustrates the
look up logic of a parameter `foo` into an example of simplified hierarchy:

[[img-hiera_layers_tree]]
.Simplified Hiera tree with specialization layers
image::src/img/hiera_layers_tree.pdf[width=600]

The typical Hiera layers to use with Puppet-HPC are the following:

* `%{cluster_name}/roles/%{puppet_role}`
* `%{cluster_name}/cluster`
* `%{cluster_name}/network`
* `organization`
* `common`

The `common` layer is directly provided by Puppet-HPC with YAML file
`hieradata/common.yaml`. The upper layers are specific to an organization or a
cluster, they must be defined in the <<arch-internalrepo,internal repository>>
documented in the next section of this documentation. Network data are separated
in a specific file (`network.yaml`) only to keep cluster YAML file readable.

The hierarchy (with all its layers) is setup in `hiera.yaml` configuration file.
An example of this file is provided with Puppet-HPC under the path
`examples/privatedata/hiera.yaml`.

[[arch-internalrepo]]
== Internal repository

////
TODO: explain Puppet-HPC must be used in combination with a another internal
repository. Highly recommended content and structure:

* internal modules
* internal files
* internal hieradata
////

Puppet-HPC cannot be used only on its own, it must be configured for a specific
site and a specific cluster. It is recommended to work with an additional
internal repository that will contain all specific data that cannot be
published on GitHub.

[[img-internal_repo_combination]]
.Combination of Puppet-HPC with internal repository
image::src/img/internal_repository_combination.pdf[width=500]

The content and structure of this internal repository is explained below:

* `files` contains any configuration file that needs to be stored internally
and that can be used as it is in a cluster configuration. It can be, for example
, SSL certificates, SSH host keys, etc. If the content of these files is
sensitive, they should be encrypted.
* `hieradata` contains all the site-specific and cluster-specific data
necessary to configure a cluster with Puppet-HPC.
* `puppet-config` includes some Puppet configuration files to use with a
specific cluster such as puppet.conf, hiera.yaml.

For each of these 3 directories, it is recommended to have a subdirectory per
cluster being configured with Puppet-HPC.

[[arch-deps]]
== External dependencies

Puppet-HPC provides a set of Puppet modules but also relies on some Puppet
external community modules that can be considered as dependencies. The full
list of these modules is:

* https://forge.puppet.com/puppetlabs/stdlib[`puppetlabs-stdlib`]
* https://forge.puppet.com/puppetlabs/concat[`puppetlabs-concat`]
* https://forge.puppet.com/puppetlabs/apache[`puppetlabs-apache`]
* https://forge.puppet.com/puppetlabs/apt[`puppetlabs-apt`]
* https://forge.puppet.com/arioch/keepalived[`arioch-keepalived`]
* https://forge.puppet.com/herculesteam/augeasproviders[`herculesteam-augeasproviders-core`]
* https://forge.puppet.com/puppet/archive[`puppet-archive`]
* https://forge.puppet.com/puppet/collectd[`puppet-collectd`]
* https://forge.puppet.com/saz/rsyslog[`saz-rsyslog`]
* https://forge.puppet.com/yo61/logrotate[`yo61-logrotate`]

It is also strongly recommended to install
https://github.com/TomPoulton/hiera-eyaml[eyaml] utility in order to encrypt
sensitive data (such as passwords) in Hiera YAML files.

[[arch-genericity]]
== Genericity levels

The use of the Roles/Profiles pattern enables to control the level of
genericity of each element of a Puppet configuration code base. Here are the
genericity levels defined for all components in the Puppet-HPC project:

* Roles: The code part of the roles is fully generic as it consists into one
  manifest (located under path `puppet-config/cluster/roles/manifests/init.pp`)
  which simply extract from Hiera the list of profiles included in the role.
  However, the name of the roles and the list of profiles are cluster specific
  due to technical properties of Puppet-HPC. This point is explained further in
  the <<arch-roles,roles section>>.
* Profiles are HPC specific and are highly related to the way Scibian HPC
  clusters are modelized. Nevertheless they can be reused from one cluster to
  the other. Their structure should follow the reference architecture defined in
  https://github.com/edf-hpc/scibian-hpc-install-guide[Scibian HPC cluster
  installation guide].
* Modules are fully generic. They support multiple distributions and can even be
  used outside of the HPC clusters context when relevant.

As stated in <<arch-hieralayers,Hiera layers section>>, the Hiera repository is
composed of multiple layers of datasets ordereded by genericity levels. Then,
each layer of the hierarchy has it own genericity level depending on its
specialization.

This diagram gives quick glance summary of the genericity levels for each
element of the Puppet-HPC stack:

[[img-components_genericity]]
.Stack components genericity goals
image::src/img/components_genericity.pdf[width=500]

[[arch-roles]]
== Roles

Each host in the cluster must provide features to fulfill its role. Each
feature is called a profile. Each role is defined by a name and a set of
profiles.

Roles themselves are defined by the name of the hosts. The name is analysed by
searching for a pattern `<prefix><role name><numerical ID>` (Example:
`gencritical1`). This is done by the `hpclib` module which defines the
`puppet_role` fact. The prefix is the hiera variable: `cluster_prefix`.

Each node has a fact `puppet_role` that is referenced in the Hiera
configuration. By doing this, it is possible to define Hiera variables with
an identical value for all the hosts with the same role. This is used to define
a Hiera variable named `profiles` that lists the names of all the profiles that
should be applied to a host given its role.

As an example, we define in `<privatedata>/hieradata/<cluster_name>/roles/cn.yaml`:

```
profiles:
  - profiles::cluster::common
  - profiles::network::base
  - profiles::dns::client
  - profiles::access::base
  - profiles::ntp::client
  - profiles::ssmtp::client
  - profiles::jobsched::exec
  - profiles::openssh::server
  - profiles::environment::base
  - profiles::environment::codes
  - profiles::environment::modules
  - profiles::environment::vtune
  - profiles::environment::limits
  - profiles::auth::client
  - profiles::metrics::collect_base
  - profiles::metrics::collect_jobsched_exec
  - profiles::hardware::user_tuning
  - profiles::hardware::ipmi
  - profiles::log::client
```

All machines sharing the same role must have the same set of profiles. If it
is necessary to apply a different set of profiles to different machines, a new
role must be created.

== Cluster definition

////
TODO: how-to write a cluster definition in hiera.

* Main shared parameters
* Network parameters
* Cluster name. Also explain why not in hiera.
////

This cluster configuration is meant to be used with a standard cluster
architecture, deviation from this architecture should be minimum. Some
constraints are planned to be relaxed in the future.

Here, we are going to describe this architecture and how it should be
defined to be used by the Puppet-HPC configuration.

[[cluster-architecture]]
=== Architecture

image::src/img/cluster_architecture.png[image]

As noted earlier, each machine is defined by its _role_ (front, cn, 
batch, etc ...). We can separate the machines into two groups : nodes with user
access (frontends, compute nodes, etc ..) and nodes who provides services for 
the cluster, including storage. There's no particular distinction (except 
the _role_) between these two groups in the configuration.

[[cluster-main-parameters]]
=== Main shared parameters

Some parameters must be defined at the _cluster_ level of the Hiera hierarchy.
These variables are not associated to a unique profile and are aimed to be
reused directly several times in Hiera or in puppet profiles.

* `cluster_name`: The complete name of the cluster. Can be used in slurm 
configuration, for example.
* `cluster_prefix`: The prefix used for all the hostnames in the cluster. 
Generally it will be composed of 2 or 3 letters ("gen" for a cluster 
named "generic", for example).
* `private_files_dir`: The directory where all the files copied by puppet on the
machines are stored. These files can be encrypted or not.
It can be a shared directory between all the nodes, or an http export if the 
_hpclib::hpc_file_ resource is used.
* `domain`: The domain name used across all the machines. Used in particular by the
_bind_ module.  
* `user_groups`: Array of user groups authorized to connect and submit jobs to
the cluster.
* `cluster_decrypt_password`: General password used by the _hpclib_ module to 
decrypt encrypted files before copying them on the machines. This variable is
usually itself encrypted using e-yaml.

[[cluster-network-definitions]]
=== Network definitions

[[cluster-topology]]
==== Topology

The network topology is defined at the _cluster_ level of the Hiera
hierarchy. This means it is common to all nodes.

----
## Network topology of the cluster
net::administration::ipnetwork: '172.16.0.0'
net::administration::netmask: '255.255.248.0'
net::administration::prefix_length: '/21'
net::administration::broadcast: '172.16.7.255'
net::lowlatency::ipnetwork: '172.16.40.0'
net::lowlatency::prefix_length: '/21'
net::management::ipnetwork: '172.16.80.0'
net::management::netmask: '255.255.240.0'
net::management::broadcast: '172.16.95.255'
net_topology:
    'wan':
        'name':           'WAN'
        'prefixes':       'wan'
        'ipnetwork':      '172.17.0.0.0'
        'netmask':        '255.255.255.0'
        'prefix_length':  '/24'
        'gateway':        '172.17.0.1'
        'broadcast':      '172.17.0.255'
        'ip_range_start': '172.17.0.1'
        'ip_range_end':   '172.17.0.254'
        'firewall_zone':  'wan'
    'administration':
        'name':           'ADM'
        'prefixes':       ''
        'ipnetwork':      '172.16.0.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':        '172.16.0.1'
        'broadcast':      '172.16.7.255'
        'ip_range_start': '172.16.0.1'
        'ip_range_end':   '172.16.7.254'
        'firewall_zone':  'clstr'
        'pool0':
            'ip_range_start':
                            '172.16.0.1'
            'ip_range_end':
                            '172.16.5.254'
        'pool1':          # IP reserved for the discovery process
            'ip_range_start':
                            '172.16.6.1'
            'ip_range_end':
                            '172.16.7.254'
    'lowlatency':
        'name':           'IB'
        'prefixes':       'ib'
        'ipnetwork':      '172.16.40.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':         ''
        'broadcast':       '172.16.47.255'
        'ip_range_start':  '172.16.40.1'
        'ip_range_end':    '172.16.47.254'
        'firewall_zone':   'clstr'
    'management':
        'name':            'MGT'
        'prefixes':        'mgt'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.240.0'
        'prefix_length':   '/20'
        'gateway':         ''
        'broadcast':       '172.16.95.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.95.254'
        'firewall_zone':   'clstr'
    'bmc':
        'name':            'BMC'
        'prefixes':        'bmc'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.248.0'
        'prefix_length':   '/21'
        'gateway':         ''
        'broadcast':       '172.16.87.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.87.254'
        'firewall_zone':   'clstr'
----

The `bmc` network connect all the management cards (bmc, imm, etc ...). The
`management` network connect the servers who must access these management
devices. That is the reason why they share an IP networks settings and ranges.

[[cluster-bonding]]
==== Bonding

Some network interfaces are bonded together for load balancing and high
availability. The bonding definition is done in Hiera. If the bonding is
uniform (i.e. same bond interface on same slave interfaces) between nodes,
this can be done at the _cluster_ level. In case of differences between
nodes, it must be done higher in the hierarchy (_role_ or _node_).

----
network::bonding_options:
    'bond0':
      'slaves':
         - 'eth0'
         - 'eth1'
      'options':  'mode=active-backup primary=eth0'
    'bond1':
      'slaves':
         - 'eth2'
         - 'eth3'
      'options':  'mode=active-backup primary=eth2'
----

This variable from Hiera uses _Auto Lookup_ to be passed to the `network`
class.

[[cluster-bridges]]
==== Bridges

When using a machine as a physical host for VMs, it is often necessary to
setup bridge interfaces. This bridge interfaces will be configured in the
`master_network` hash with the right IP addresses. The physical device will be
added automatically without an IP address. It is possible to create a bridge
above a bonding interface.

----
network::bridge_options:
    'br0':
      'ports':
         - 'bond0'
      'description': 'Administration network bridge.'
    'br2':
      'ports':
         - 'bond2'
      'description': 'WAN network bridge'
----

[[cluster-node-definitions]]
=== Node definitions

==== Master Network

Nodes are listed in a Hiera hash called `master_network`. It must be defined 
at the at the _cluster_ level of the Hiera hierarchy, but for readability
reasons it is in a separate `network.yaml` file.
Each key defines one node and its network configuration. Each node is described
by a hash containing its fully qualified hostname and the networks attached to 
it. Each network must have a name corresponding to the ones used in the  
`net_topology` hash described in the <<cluster-topology,topology section>>.

image::src/img/net_topology-master_network-corresponding.png[image]

These values can be defined for each network:

* MAC address (`DHCP_MAC`):
The MAC address of the physical device connected to the network. It is used
to build the dhcpd server configuration.

* Interface device (`device`):
The device where the configuration must be applied, this means that with a
bonded interface, the configuration must be applied on the `bond` interface.
So it is not necessary the physical interface is attached to the MAC address
quoted above. The interfaces enslaved to the `bond` interfaces can be
omitted from this configuration.

* Hostname (`hostname`):
The hostname of the machine on the considered network.

* IPv4 Address (`IP`):
The IPv4 address of the machine on the considered network. The netmask
comes from the `net_topology` variable.

* External config (or not) :
External configuration means the interface is configured on the system but
should not be setup by the Puppet-HPC configuration. It is useful if another
subsystem sets up the network interface: VPN, libvirt... On Debian, it means
the interface is not added to `/etc/network/interfaces`. This boolean can take
the value `true` or `false` and is considered `false` if omitted

Example:

----
master_network:
  genmisc1:
    fqdn: 'genmisc1.somedomain'
    networks:
      administration:
        'DHCP_MAC': '52:54:00:ba:9d:ac'
        'IP':       '172.16.2.21'
        'device':   'bond0'
        'hostname': 'genmisc1'
      lowlatency:
        'IP':       '172.16.42.21'
        'device':   'ib0'
        'hostname': 'llgenmisc1'
      management:
        'IP':       '172.16.88.21'
        'device':   'bond0'
        'hostname': 'mgtgenmisc1'
      bmc:
        'DHCP_MAC': '40:F2:E9:CD:53:CE'
        'IP':       '172.16.82.21'
        'hostname': 'bmcgenmisc1'
      wan:
        'IP':       '172.17.42.45'
        'device':   'bond1'
        'hostname': 'extgenmisc1'
----

This example defines one node (`genmisc1`) with the following
configuration:

* DHCP
** `52:54:00:ba:9d:ac` `genmisc1` `172.16.2.21`
** `40:F2:E9:CD:53:CE` `mgtgenmisc1` `172.16.82.21`
* Network configuration on the node
** `bond0` `172.16.2.21` `255.255.248.0` External Config: `false`
** `bond0` `172.16.88.21` `255.255.248.0` External Config: `false`
** `bond1` `172.17.42.45` `255.255.255.0` External Config: `false`
* DNS and Hosts
** `genmisc1` `172.16.2.21`
** `extgenmisc1` `172.17.42.45`

All lists are optional, so it is possible to define an element that just
defines a Host/DNS configuration (for virtual IP addresses for
instance):

----
master_network:
  genmisc:
    fqdn: 'genmisc.somedomain'
    networks:
      administration:
        'IP':       '172.16.2.20'
        'hostname': 'genmisc'
      management:
        'IP':       '172.16.82.20'
        'hostname': 'mgtgenmisc'
      wan:
        'IP':       '172.17.42.44'
        'hostname': 'extgenmisc'
----

== Deployment
////
TODO: document installation mode (packages, git clones) and push and apply
scripts with refs to manpages. Explain why not puppetserver mode.
////

Scibian clusters uses a simple _puppet apply_ command with a set of modules,
manifests and data. Puppet-hpc is not designed to work in a traditional
"Puppet server" environment, as it must be used in a very simple system
environment, like a post installation script inside Debian Installer.

=== Push and apply scripts

Two tools have been developed in order to apply the puppet-hpc configuration
on the nodes of a cluster. One "push" the entire configuration (modules, Hiera
data, files) in one shared space, and another one is aimed to apply this
configuration on the nodes.

image::src/img/config_push_apply.png[image]

==== hpc-config-push

This script merge all the data necessary to apply the puppet configuration in 
one archive and push it into a shared storage:

* Puppet modules installed in operating system via packages
* Puppet modules from the _puppet_hpc_ git repository, including profiles
* Optionally other puppet modules
* Configuration files for puppet and Hiera
* Yaml files for Hiera: generic ones from puppet-hpc git repository and
  specific ones from the internal repository
* Files to copy on nodes from the internal repository

Two methods can be used to push the data:
* *posix*: simply copies the tarball into a shared directory on all nodes
  (a nfs export, for example)
* *s3*: uses the Amazon S3 RESTful API to send data on a compatible storage
  (Ceph Rados Gateway, for example)

The script can manage several
https://docs.puppet.com/puppet/3.7/environments.html[Puppet environments]
with the `-e, --environment` parameter. A default environment can be defined in
the configuration file.

The file `/etc/hpc-config/push.conf` allows to configure all the options for
this script. See manpage for detailed description.

Please refer to `hpc-config-push(1)` manpage for full usage documentation.

==== hpc-config-apply

The apply script downloads the puppet configuration (modules and hieradata)
as a tarball and installs it as an environment in `/etc/puppet`. Private data
files are not downloaded with the configuration. Private data files are
available from the central storage and are directly downloaded from the puppet
modules.
If eyaml is used, this script needs a source to download the keys used to
encrypt and decrypt data.

The command `puppet apply` is executed afterward with the environment
previously untarred.

The configuration file indicating where to download the files is located in
_/etc/hpc-config.conf_. See the manpage for detailed description of the
options.

Please refer to `hpc-config-apply(1)` manpage for full usage documentation.

=== Packages

These two scripts are provided with Scibian distribution as debian packages for
Debian Jessie:

* `hpc-config-apply`
* `hpc-config-push`
