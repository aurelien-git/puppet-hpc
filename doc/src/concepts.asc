= Concepts

== Overview

Puppet-HPC is a Puppet Configuration designed to easily be used for HPC
clusters. The main goal of Puppet-HPC is to provide a generic configuration
that can be reused effortlessly for different clusters or sites. This implies a
strict separation between cluster-specific or site-specific data from Puppet code.

Puppet-HPC is developed and actively maintained by the CCN-HPC (Centre de
Compétences Nationales en High Performance Computing) of https://www.edf.fr[EDF] (Electrité de
France).

Any external contribution is very welcome and should be made under
the form of a pull request or an issue creation to our Github repository.
Please refer to the <<devgl, Development Guidelines>> if you consider contributing.

== Software architecture

////
TODO: Give some insights about Puppet-HPC software architecture and its inner
workings.

Based on roles and profiles pattern, with hiera backend.

[schema]

Based on some external community modules:

* stdlib
* apache2
* eyaml
////

Puppet-HPC is based on the 3 following tools and principles:

* *Facter* is used to report per-node facts. Morevover some facts specific to
the HPC context are used to convey the global information about the cluster
that needs to be known when running Puppet on a node. These facts are
implemented in the `hpclib` module.

* *Hiera* is used to look up data. This tool helps separating site-specific or
cluster-specific data from Puppet code. Though specific data is excluded from
Puppet-HPC, being kept, versionned and maintained in a separate internal
repository, the cluster description it contains should follow certain rules.
These rules are detailed below in the Cluster Definition part.

* *The Roles/Profiles pattern* has been used to design the Puppet-HPC code. It
is organized in different levels of abstraction:

** Roles, which represent the business logic. A node includes one role,
and one only. Each role lists one or more profiles.
** Profiles, which represent the implementation of technical functionalities.
A profile includes and manages modules to define a logical technical stack.
** Modules, which are the elementary technical blocks. Modules should only be
responsible for managing aspects of the component they are written for and
should be as generic as possible.

Regarding the Roles/Profiles pattern, it is a common pattern in Puppet code
organization. This pattern is explained in details in this presentation:
https://puppet.com/presentations/designing-puppet-rolesprofiles-pattern

image::src/img/the-stack.jpg[image, title="The stack (from 'Designing Puppet: Roles/Profiles Pattern' by Craig Dunn)"]

One of the interesting aspects of the Roles/Profiles pattern is that modules
should be as generic as possible. Whenever it is possible, *external community
modules* should be used. It should come from a reliable source: distribution
package or the Puppetlabs forge. In any case, external community modules should
be properly reviewed.

== Why a Generic Puppet Code?

////
TODO: explain project genericity goal. First draft could be something like:

* Modules fully generic, support multiple distributions and can be used outside
  of HPC context
* Hieradata and profiles generic to all Scibian HPC clusters following the
  reference architecture defined in [Scibian HPC cluster installation guide]
* Roles (with names and associated profiles) are specific to each cluster they
  are not defined nor provided by puppet-hpc.
////

The design of Puppet-HPC uses a standardized description of an HPC cluster and
takes into account the evolution of Puppet standards to keep the Puppet code
as generic as possible. This is crucial in order to:

* be able to reuse the code from one cluster to the other,
* operate and maintain the cluster effortlessly,
* test the Puppet code on a test environment even if the data is different than
in the production environment.

The use of the Roles/Profiles pattern enables to control the degree of genericity
of each level of abstraction in the Puppet Code.

* Roles (with names and associated profiles) are specific to each cluster. They
 are not defined nor provided with Puppet-HPC.
* Profiles are HPC specific and are highly related to the way Scibian HPC clusters
are modelized. Nevertheless they can be reused from one cluster to the other. Their
structure should follow the reference architecture defined in [Scibian HPC cluster
installation guide].
* Modules are fully generic. Ideally they can support multiple distributions and
can be used outside of the HPC context.

Hiera makes it possible to keep data out of manifests and Puppet Code.
It allows classifying the data in different levels of specifity: common data,
site-specific data, cluster-specific data and so on.

image::src/img/generic_code.png[image, title="Code and Data Separation with Puppet-HPC"]

== Internal repository

////
TODO: explain Puppet-HPC must be used in combination with a another internal
repository. Highly recommended content and structure:

* internal modules
* internal files
* internal hieradata
////

Puppet-HPC cannot be used only on its own, it must be configured for a specific
site and a specific cluster. It is recommended to work with an additional
internal repository that will contain all specific data that cannot be
published on Github.

The content and structure of this internal repository is explained below:

* `files` contains any configuration file that needs to be stored internally
and that can be used as it is in a cluster configuration. It can be, for example
, SSL certificates, SSH host keys, ect. If the content of these files is
sensitive, they should be encrypted.
* `hieradata` contains all the site-specific and cluster-specific data
necessary to configure a cluster with Puppet-HPC.
* `puppet-config` includes some Puppet configuration files to use with a
specific cluster such as puppet.conf, hiera.yaml.

For each of these 3 directories, it is recommended to have a subdirectory per
cluster being configured with Puppet-HPC.

== Hieradata levels

Hiera uses an ordered hierarchy to look up data. This allows you to have a large
amount of common data and override smaller amounts of it wherever necessary.

Hiera loads the hierarchy from the hiera.yaml config file. It defines different
levels of data.

Here an example of hierarchy who can be used :

* private/%{cluster_name}/roles/%{puppet_role}
* private/%{cluster_name}/cluster
* private/%{cluster_name}/network
* private/organization
* generic/common
* generic/%{osfamily}/common

The files from the `generic` directory come from the puppet-hpc configuration, the
files from the `private` directory are specific to an organization or a cluster.

Network data are in a specific file only to keep cluster yaml file readable.

An example of hiera.yaml with the typical hierarchy used for Puppet-HPC can be found
in the `examples/private-data` directory.

== Roles

////
TODO: explain that roles are defined in hiera. The role associated to a node is
extracted from its hostname.
////

Each host in the cluster must provide features to fullfill its role. Each
feature is called a profile. Each role is defined by a name and a set of
profiles.

Roles themselves are defined by the name of the hosts. The name is analysed by
searching for a pattern `<prefix><role name><numerical ID>` (Example:
`gencritical1`). This is done by the `hpclib` module which defines the
`puppet_role` fact. The prefix is the hiera variable: `cluster_prefix`.

Each node has a fact `puppet_role` that is referenced in the Hiera
configuration. By doing this, it is possible to define Hiera variables with
an identical value for all the hosts with the same role. This is used to define
a Hiera variable named `profiles` that lists the names of all the profiles that
should be applied to a host given its role.

As an example, we define in `<privatedata>/hieradata/<cluster_name>/roles/cn.yaml`:

```
profiles:
  - profiles::cluster::common
  - profiles::network::base
  - profiles::dns::client
  - profiles::access::base
  - profiles::ntp::client
  - profiles::ssmtp::client
  - profiles::jobsched::exec
  - profiles::openssh::server
  - profiles::environment::base
  - profiles::environment::codes
  - profiles::environment::modules
  - profiles::environment::vtune
  - profiles::environment::limits
  - profiles::auth::client
  - profiles::metrics::collect_base
  - profiles::metrics::collect_jobsched_exec
  - profiles::hardware::user_tuning
  - profiles::hardware::ipmi
  - profiles::log::client
```

All machines sharing the same role, must have the same set of profiles. If it
is necessary to apply a different set of profiles to different machines, it
means, a new role must be created.

== Cluster definition

////
TODO: how-to write a cluster definition in hiera.

* Main shared parameters
* Network parameters
* Cluster name. Also explain why not in hiera.
////

This cluster configuration is meant to be used with a standard cluster
architecture, deviation from this architecture should be minimum. Some
constraints are planned to be relaxed in the future.

Here, we are going to describe this architecture and how it should be
defined to be used by the Puppet-HPC configuration.

[[cluster-architecture]]
=== Architecture

image::src/img/cluster_architecture.png[image]

[[cluster-network-definitions]]
=== Network definitions

[[cluster-topology]]
==== Topology

The network topology is defined at the _cluster_ level of the Hiera
hierarchy. This means it is common to all nodes.

-----------------------------------------------------------------
## Network topology of the cluster
net::administration::ipnetwork: '172.16.0.0'
net::administration::netmask: '255.255.248.0'
net::administration::prefix_length: '/21'
net::administration::broadcast: '172.16.7.255'
net::lowlatency::ipnetwork: '172.16.40.0'
net::lowlatency::prefix_length: '/21'
net::management::ipnetwork: '172.16.80.0'
net::management::netmask: '255.255.240.0'
net::management::broadcast: '172.16.95.255'
net_topology:
    'wan':
        'name':           'WAN'
        'prefixes':       'wan'
        'ipnetwork':      '172.17.0.0.0'
        'netmask':        '255.255.255.0'
        'prefix_length':  '/24'
        'gateway':        '172.17.0.1'
        'broadcast':      '172.17.0.255'
        'ip_range_start': '172.17.0.1'
        'ip_range_end':   '172.17.0.254'
        'firewall_zone':  'wan'
    'administration':
        'name':           'ADM'
        'prefixes':       ''
        'ipnetwork':      '172.16.0.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':        '172.16.0.1'
        'broadcast':      '172.16.7.255'
        'ip_range_start': '172.16.0.1'
        'ip_range_end':   '172.16.7.254'
        'firewall_zone':  'clstr'
        'pool0':
            'ip_range_start':
                            '172.16.0.1'
            'ip_range_end':
                            '172.16.5.254'
        'pool1':          # IP reserved for the discovery process
            'ip_range_start':
                            '172.16.6.1'
            'ip_range_end':
                            '172.16.7.254'
    'lowlatency':
        'name':           'IB'
        'prefixes':       'ib'
        'ipnetwork':      '172.16.40.0'
        'netmask':        '255.255.248.0'
        'prefix_length':  '/21'
        'gateway':         ''
        'broadcast':       '172.16.47.255'
        'ip_range_start':  '172.16.40.1'
        'ip_range_end':    '172.16.47.254'
        'firewall_zone':   'clstr'
    'management':
        'name':            'MGT'
        'prefixes':        'mgt'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.240.0'
        'prefix_length':   '/20'
        'gateway':         ''
        'broadcast':       '172.16.95.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.95.254'
        'firewall_zone':   'clstr'
    'bmc':
        'name':            'BMC'
        'prefixes':        'bmc'
        'ipnetwork':       '172.16.80.0'
        'netmask':         '255.255.248.0'
        'prefix_length':   '/21'
        'gateway':         ''
        'broadcast':       '172.16.87.255'
        'ip_range_start':  '172.16.80.1'
        'ip_range_end':    '172.16.87.254'
        'firewall_zone':   'clstr'
-----------------------------------------------------------------

The `bmc` network connect all the management cards (bmc, imm, etc ...). The
`management` network connect the servers who must access these management
cards. That is why they share an IP range.

[[cluster-bonding]]
==== Bonding

Some network interfaces are bonded together for load balancing and high
availability. The bonding definition is done in Hiera. If the bonding is
uniform (i.e. same bond interface on same slave interfaces) between nodes,
this can be done at the _cluster_ level. In case of differences between
nodes, it must be done higher in the hierarchy (_role_ or _node_).

---------------------------------------------------
network::bonding_options:
    'bond0':
      'slaves':
         - 'eth0'
         - 'eth1'
      'options':  'mode=active-backup primary=eth0'
    'bond1':
      'slaves':
         - 'eth2'
         - 'eth3'
      'options':  'mode=active-backup primary=eth2'
---------------------------------------------------

This variable from Hiera uses _Auto Lookup_ to be passed to the `network`
class.

[[cluster-bridges]]
==== Bridges

When using a machine as a physical host for VMs, it is often necessary to
setupe bridge interfaces. This bridge interface will be configured in the
`master_network` hash with the right IP addresses. The physical device will be
added automatically without an IP address. It is possible to create a bridge
above a bonding interface.

--------------------------------------------------
network::bridge_options:
    'br0':
      'ports':
         - 'bond0'
      'description': 'Administration network bridge.'
    'br2':
      'ports':
         - 'bond2'
      'description': 'WAN network bridge'
--------------------------------------------------

[[cluster-node-definitions]]
=== Node definitions

==== Master Network

Nodes are listed in a Hiera hash called `master_network`. Each key
defines one node and its network configuration.

Each node is described by a hash containing its full qualified hostname
and the networks attached to it.

Each network must have a name corresponding to the ones used in the
`net_topology` hash described above.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
master_network:                     |
  genmisc1:                         |
    fqdn: 'genmisc1.somedomain'     |
    networks:                       |  net_topology:
      wan: =========================|====> 'wan':
        'IP':       '172.17.42.45'  |          'name':           'WAN'
        'device':   'bond1'         |          'prefixes':       'wan'
        'hostname': 'extgenmisc1'   |          'ipnetwork':      '172.17.0.0.0'
  ....                              |          'netmask':        '255.255.255.0'
                                    |          'prefix_length':  '/24'
                                    |          'gateway':        '172.17.0.1'
                                    |          'broadcast':      '172.17.0.255'
                                    |          'ip_range_start': '172.17.0.1'
                                    |          'ip_range_end':   '172.17.0.254'
                                    |          'firewall_zone':  'wan'
                                    |   ....
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

These values can be defined for each network:

* MAC address (`DHCP_MAC`):
The MAC address of the physical device connected to the network. It is used
to build the dhcpd server configuration.

* Interface device (`device`):
The device where the configuration must be applied, this means that with a
bonded interface, the configuration must be applied on the `bond` interface.
So it is not necessary the physical interface is attached to the MAC address
quoted above. The interfaces enslaved to the `bond` interfaces can be
ommited from this configuration.

* Hostname (`hostname`):
The hostname of the machine on the considered network.

* IPv4 Address (`IP`):
The IPv4 address of the machine on the considered network. The netmask
comes from the `net_topology` variable.

* External config (or not) :
External configuration means the interface is configured on the system but
should not be setup by the Puppet-HPC configuration. It is usefull if another
subsystem sets up the network interface: VPN, libvirt... On Debian, it means
the interface is not added to `/etc/network/interfaces`. This boolean can take
the value `true` or `false` and is considered `false` if ommited.

Example:

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
master_network:
  genmisc1:
    fqdn: 'genmisc1.somedomain'
    networks:
      administration:
        'DHCP_MAC': '52:54:00:ba:9d:ac'
        'IP':       '172.16.2.21'
        'device':   'bond0'
        'hostname': 'genmisc1'
      lowlatency:
        'IP':       '172.16.42.21'
        'device':   'ib0'
        'hostname': 'llgenmisc1'
      management:
        'IP':       '172.16.88.21'
        'device':   'bond0'
        'hostname': 'mgtgenmisc1'
      bmc:
        'DHCP_MAC': '40:F2:E9:CD:53:CE'
        'IP':       '172.16.82.21'
        'hostname': 'bmcgenmisc1'
      wan:
        'IP':       '172.17.42.45'
        'device':   'bond1'
        'hostname': 'extgenmisc1'
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

This example defines one node (`genmisc1`) with the following
configuration:

* DHCP
** `52:54:00:ba:9d:ac` `genmisc1` `172.16.2.21`
** `40:F2:E9:CD:53:CE` `mgtgenmisc1` `172.16.82.21`
* Network configuration on the node
** `bond0` `172.16.2.21` `255.255.248.0` External Config: `false`
** `bond0` `172.16.88.21` `255.255.248.0` External Config: `false`
** `bond1` `172.17.42.45` `255.255.255.0` External Config: `false`
* DNS and Hosts
** `genmisc1` `172.16.2.21`
** `extgenmisc1` `172.17.42.45`

All lists are optional, so it is possible to define an element that just
defines a Host/DNS configuration (for virtual IP addresses for
instance):

--------------------------------------------------------------------------------------------------------------------------------------------
master_network:
  genmisc:
    fqdn: 'genmisc.somedomain'
    networks:
      administration:
        'IP':       '172.16.2.20'
        'hostname': 'genmisc'
      management:
        'IP':       '172.16.82.20'
        'hostname': 'mgtgenmisc'
      wan:
        'IP':       '172.17.42.44'
        'hostname': 'extgenmisc'
--------------------------------------------------------------------------------------------------------------------------------------------


== Deployment

////
TODO: document installation mode (packages, git clones) and push and apply
scripts with refs to manpages. Explain why not puppetserver mode.
////
